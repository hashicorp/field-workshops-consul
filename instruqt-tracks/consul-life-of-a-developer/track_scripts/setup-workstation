#!/bin/bash

#fix path
echo "export PATH=/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" >> ~/.bashrc

#install kubectl
export DEBIAN_FRONTEND=noninteractive
apt update -y
apt install -y apt-transport-https ca-certificates curl vim openssh-client git jq gnupg2 lsb-release software-properties-common
curl -fsSL https://apt.releases.hashicorp.com/gpg | apt-key add -
add-apt-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" |  tee /etc/apt/sources.list.d/kubernetes.list
apt update -y
apt install -y kubectl consul=$CONSUL_VERSION vault

#assets
git clone https://github.com/hashicorp/field-workshops-consul.git
cp -r field-workshops-consul/instruqt-tracks/consul-life-of-a-developer/assets/deployments /root/
cp -r field-workshops-consul/instruqt-tracks/consul-life-of-a-developer/assets/helm /root/
rm -rf /root/field-workshops-consul

#disable host checks
cat << EOF > ~/.ssh/config
Host *
    StrictHostKeyChecking no
EOF

#get the kube configs
ssh k8s1 'cat ~/.kube/config' > /tmp/k8s1_kube_config.yaml
ssh k8s2 'cat ~/.kube/config' > /tmp/k8s2_kube_config.yaml

#name them - k8s1
sed -i 's/default/k8s1/g' /tmp/k8s1_kube_config.yaml
sed -i 's/127.0.0.1/k8s1/g' /tmp/k8s1_kube_config.yaml
#name them - k8s2
sed -i 's/default/k8s2/g' /tmp/k8s2_kube_config.yaml
sed -i 's/127.0.0.1/k8s2/g' /tmp/k8s2_kube_config.yaml

#merge them & set the context
mkdir -p ~/.kube/
KUBECONFIG=/tmp/k8s1_kube_config.yaml:/tmp/k8s2_kube_config.yaml kubectl config view --merge --flatten > ~/.kube/config
kubectl config use-context k8s1

#helm install
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
/usr/local/bin/helm repo add hashicorp https://helm.releases.hashicorp.com

#nginx
export DEBIAN_FRONTEND=noninteractive
apt update -y
apt install -y nginx
cat << EOF > /etc/nginx/conf.d/consul.conf
server {
  listen 8500;
  location / {
    proxy_pass https://k8s1:8443;
    proxy_ssl_verify off;
  }
  access_log /var/log/nginx/consul.log;
}
EOF
/usr/sbin/service nginx restart

#vault deploy
/usr/sbin/setcap cap_ipc_lock= /usr/bin/vault
kubectl config use-context k8s1
/usr/local/bin/helm install vault hashicorp/vault -f /root/helm/vault-values.yaml
sleep 60

#vault config
export VAULT_ADDR=http://k8s1:8200
/usr/bin/vault login root
/usr/bin/vault secrets enable transit
/usr/bin/vault write -f transit/keys/payments
/usr/bin/vault auth enable kubernetes
secret=$(kubectl get sa vault -o json | jq -r .secrets[0].name)
kubectl get secret ${secret} -o json | jq -r '.data."ca.crt"' | base64 -d > /tmp/ca.pem
/usr/bin/vault write auth/kubernetes/config \
    token_reviewer_jwt="$(kubectl get secret ${secret} -o json | jq -r .data.token | base64 -d)" \
    kubernetes_host="https://$(kubectl get svc kubernetes -o json | jq -r .spec.clusterIP):443" \
    kubernetes_ca_cert=@/tmp/ca.pem
/usr/bin/vault policy write payments-api - <<EOF
path "transit/encrypt/payments" {
  capabilities = ["update"]
}
EOF
/usr/bin/vault write auth/kubernetes/role/payments-api \
    bound_service_account_names=payments-api \
    bound_service_account_namespaces=default \
    policies=payments-api \
    ttl=24h

#grafana
kubectl config use-context k8s1
cat <<-EOF > /root/grafana-values.yaml
persistence:
  enabled: false
datasources:
  datasources.yaml:
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        url: http://prometheus-server
        access: proxy
        isDefault: true
EOF
cat <<EOF | kubectl apply -f -
---
apiVersion: v1
kind: Service
metadata:
  name: grafana-service
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: grafana
  ports:
  - name: http
    protocol: TCP
    port: 3000
    targetPort: 3000
EOF
helm repo add grafana https://grafana.github.io/helm-charts
helm install grafana grafana/grafana -f /root/grafana-values.yaml --wait --debug
sleep 15
kubectl get secret --namespace default grafana -o jsonpath="{.data.admin-password}" | base64 --decode > /tmp/grafana-pass.txt

#tracing
kubectl config use-context k8s1
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/v1.28.0/deploy/crds/jaegertracing.io_jaegers_crd.yaml
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/v1.28.0/deploy/service_account.yaml
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/v1.28.0/deploy/role.yaml
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/v1.28.0/deploy/role_binding.yaml
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/v1.28.0/deploy/operator.yaml
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/v1.28.0/deploy/cluster_role.yaml
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/v1.28.0/deploy/cluster_role_binding.yaml
sleep 15
kubectl apply -f - <<EOF
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: jaeger-operator
subjects:
- kind: ServiceAccount
  name: jaeger-operator
  namespace: default
roleRef:
  kind: ClusterRole
  name: jaeger-operator
  apiGroup: rbac.authorization.k8s.io
EOF
sleep 60
kubectl wait pod --for=condition=Ready --selector=name=jaeger-operator --timeout=120s
kubectl apply -f - <<EOF
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger
spec:
  strategy: allInOne
  allInOne:
    image: jaegertracing/all-in-one:latest
    options:
      collector.zipkin.host-port: 9411
EOF
sleep 60
kubectl wait pod --for=condition=Ready --selector=app=jaeger --timeout=120s
cat <<EOF | kubectl apply -f -
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-service
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: jaeger
  ports:
  - name: http
    protocol: TCP
    port: 16686
    targetPort: 16686
EOF
sleep 15

#remove traefik
echo "Removing Traefik..."
kubectl config use-context k8s1
kubectl delete deployment traefik -n kube-system
kubectl delete svc traefik -n kube-system
kubectl config use-context k8s2
kubectl delete deployment traefik -n kube-system
kubectl delete svc traefik -n kube-system

#deploy consul
echo "Deploying Consul..."
#k8s1
kubectl config use-context k8s1
helm install -f /root/helm/k8s1-consul-values.yaml consul hashicorp/consul  --version $CONSUL_HELM_VERSION --wait --debug
kubectl get secret consul-federation -o yaml > consul-federation-secret.yaml
#k8s2
kubectl config use-context k8s2
kubectl apply -f consul-federation-secret.yaml
helm install -f /root/helm/k8s2-consul-values.yaml consul hashicorp/consul  --version $CONSUL_HELM_VERSION --wait --debug

#creating dashboard tokens
echo "Creating Dashboard Tokens..."
kubectl config use-context k8s1
kubectl get secret -n kubernetes-dashboard $(kubectl get sa -n kubernetes-dashboard admin-user -o json | jq -r .secrets[0].name) -o json | jq -r .data.token | base64 -d > /root/k8s1-dashboard-token.txt
kubectl config use-context k8s2
kubectl get secret -n kubernetes-dashboard $(kubectl get sa -n kubernetes-dashboard admin-user -o json | jq -r .secrets[0].name) -o json | jq -r .data.token | base64 -d > /root/k8s2-dashboard-token.txt

#pre-flight checks
echo "Running pre-flight checks..."

#k8s1
kubectl config use-context k8s1
status=$(helm status consul -o json | jq -r .info.status)
if [ "$status" != "deployed" ]; then
  fail-message "Your Consul cluster is not deployed with Helm."
  exit 1
fi
leader=$(curl -s -o /dev/null -w "%{http_code}" http://127.0.0.1:8500/v1/status/leader)
if [ "$leader" != "200" ]; then
  fail-message "Your Consul cluster is does not have a leader."
  exit 1
fi

#k8s2
kubectl config use-context k8s2
status=$(helm status consul -o json | jq -r .info.status)
if [ "$status" != "deployed" ]; then
  fail-message "Your Consul cluster is not deployed with Helm."
  exit 1
fi
leader=$(curl -s -o /dev/null -w "%{http_code}" http://127.0.0.1:8500/v1/status/leader?dc=k8s2)
if [ "$leader" != "200" ]; then
  fail-message "Your Consul cluster is does not have a leader."
  exit 1
fi

exit 0
