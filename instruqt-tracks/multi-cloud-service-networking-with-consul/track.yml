slug: multi-cloud-service-networking-with-consul
id: iklbsgjpisau
type: track
title: Multi Cloud Service Networking with Consul
teaser: Deploy a multi-cloud service mesh solution with Consul
description: Run a shared Consul service across multiple clouds and enable self-service
  for application teams.
icon: https://storage.googleapis.com/instruqt-frontend/assets/hashicorp/tracks/consul.png
tags:
- consul
- vault
- terraform
- k8s
- nomad
owner: hashicorp
developers:
- lance@hashicorp.com
private: true
published: true
show_timer: true
skipping_enabled: true
challenges:
- slug: 01-provision-cloud-infra
  id: bmfubswpxeni
  type: challenge
  title: Provision Cloud Infrastructure
  teaser: Create cloud VPCs
  notes:
  - type: text
    contents: |
      You are being provisioned on-demand cloud infrastructure. <br>
      Please be patient as this can take up to ~15 minutes.
  assignment: |
    The terraform code will provision cloud infrastructure in AWS, GPC, and Azure. <br>

    Your lab environment will leverage pre-built packer images.
    You can inspect the image build in the code editor, and validate the images are available in AWS & Azure. <br>

    ```
    az image list -g packer | jq
    aws ec2 describe-images --owners self | jq
    ```

    Inspect the terraform code and validate the VPCs and VNets that were pre-provisioned for shared services and application workloads. <br>

    ```
    aws ec2 describe-vpcs
    gcloud compute networks list
    az network vnet list
    ```

    If you get an error on your Packer images, you can rebuild either image below from the packer directory:

    ```
    cd /root/packer/
    ```

    * AWS - `packer build -force -only=amazon-ebs-ubuntu-bionic hashistack.json`
    * AZURE - `packer build -force -only=azure-ubuntu-bionic hashistack.json`

    In the next few challenges we will centralize secrets across environments.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/infra
  - title: Packer
    type: code
    hostname: cloud-client
    path: /root/packer
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  difficulty: basic
  timelimit: 300
- slug: 02-provision-vault
  id: wicxiidlfod9
  type: challenge
  title: Provision Vault Infrastructure
  teaser: Set up Vault Infrastructure and Enable Replication
  assignment: |-
    Vault supports cross-site replication. In this assignment you provision Vault infrastructure in AWS & Azure. <br>

    Inspect the Terraform code and provision the Vault infrastructure.

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    You can monitor the init scripts with the below commands. <br>


    * AWS Vault - `ssh ubuntu@$(terraform output aws_vault_ip) 'tail -f /var/log/cloud-init-output.log'`
    * Azure Vault - `ssh ubuntu@$(terraform output azure_vault_ip) 'tail -f /var/log/cloud-init-output.log'`

    Vault will support secure introduction of non-container Consul infrastructure in the next few assignments.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vault
  difficulty: basic
  timelimit: 300
- slug: 03-provision-identities
  id: jmavdzjkybyw
  type: challenge
  title: Provision Service & Workload Identities
  teaser: Create workload identities for secure introduction
  assignment: |-
    In this assignment you will create trust with various runtime platforms. These identities are required for Vault to securely introduce workloads. <br>

    You can read more about the supported Vault authentication mechanisms on the [Vault website](https://www.vaultproject.io/docs/auth). <br>

    Inspect the Terraform and create the workload identities. <br>

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    In the next assignment you will centralize secrets around these identities.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/iam
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  difficulty: basic
  timelimit: 300
- slug: 04-centralize-secrets-in-vault
  id: vsfysnyjg9od
  type: challenge
  title: Centralize Secrets in Vault
  teaser: Create trust and seed bootstrap credentials.
  assignment: |-
    In this assignment you will centralize secrets in Vault that will be consumed by applications and services across clouds and runtimes. <br>

    The Vault servers are now available and the UI can be accessed in the tab. <br>

    * AWS Vault - `ssh ubuntu@$(terraform output aws_vault_ip)`
    * Azure Vault - `ssh ubuntu@$(terraform output azure_vault_ip)`

    Check the replication status. <br>

    ```
    vault read -format=json sys/replication/status | jq
    ```

    Vault replication is configured across AWS & Azure.
    See the [documentation](https://www.vaultproject.io/docs/enterprise/replication) for more information about Vault replication. <br>

    An admin account in Vault is now available. Use the below command to log in. <br>

    ```
    vault login -method=userpass username=admin password=admin
    ```

    You can also view the [auth methods](https://www.vaultproject.io/docs/auth) and [secrets engines](https://www.vaultproject.io/docs/secrets) that are configured for the environments.

    ```
    vault secrets list
    vault auth list
    ```

    Inspect the role bindings that are configured for the Consul server infrastructure.

    ```
    vault read /auth/aws/role/consul
    vault read /auth/azure/role/consul
    ```

    Now that the configuration is valid, seed the initial secret for Consul.

    ```
    vault kv put kv/consul \
      master_token=$(cat /proc/sys/kernel/random/uuid) \
      gossip_key=$(consul keygen) \
      ttl=5m
    ```

    Last, configure Vault to support Auto Config in the AWS DC. Auto Config is out of scope for this lab in Azure DC.

    ```
    vault write /auth/aws/config/identity iam_alias=full_arn
    vault write identity/oidc/key/consul allowed_client_ids=consul-server-aws-us-east-1
    vault write identity/oidc/role/consul-aws-us-east-1 ttl=30m key=consul client_id=consul-server-aws-us-east-1 template='{"consul": {"node_arn": {{identity.entity.aliases.'$(vault auth list -format=json | jq -r '."aws/".accessor')'.name}} } }'
    ```

    **NOTE: IF YOU RECEIVE ERRORS YOU CAN RUN THE FOLLOWING TWO SCRIPTS TO REPAIR YOUR VAULT ENVIRONMENT! THIS SCRIPT IS NOT NEEDED IF YOUR SANDBOX IS WORKING CORRECTLY!!**

    ```
    #reset vault
    /root/terraform/vault/reset_vault.sh
    /root/terraform/vault/setup_vault.sh

    #configure vault
    vault login -method=userpass username=admin password=admin
    vault kv put kv/consul \
      master_token=$(cat /proc/sys/kernel/random/uuid) \
      gossip_key=$(consul keygen) \
      ttl=5m
    vault write /auth/aws/config/identity iam_alias=full_arn
    vault write identity/oidc/key/consul allowed_client_ids=consul-server-aws-us-east-1
    vault write identity/oidc/role/consul-aws-us-east-1 ttl=30m key=consul client_id=consul-server-aws-us-east-1 template='{"consul": {"node_arn": {{identity.entity.aliases.'$(vault auth list -format=json | jq -r '."aws/".accessor')'.name}} } }'
    ```

    You will tie these tokens to Consul policies in a later assignment.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vault
  - title: Vault Setup Script
    type: code
    hostname: cloud-client
    path: /root/terraform/vault/setup_vault.sh
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  difficulty: basic
  timelimit: 300
- slug: 05-provision-aws-consul-primary
  id: bt6k0zty1tyv
  type: challenge
  title: Provision AWS Consul Primary Datacenter
  teaser: Run Consul in AWS
  assignment: |-
    In this assignment you will bootstrap the initial cluster and validate the health of the server.
    The primary Consul server cluster resides in AWS. <br>

    Inspect the initialization scripts in the CLI or UI.

    ```
    cat scripts/aws_consul_server.sh
    cat scripts/aws_mesh_gateway.sh
    ```

    Inspect the Terraform and provision the servers.

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    You can monitor the init scripts with the below commands. <br>

    **NOTE: YOU WILL SEE SECRETS ERRORS ON THE MGW. THIS IS OKAY!** <br>


    * Consul Server - `ssh ubuntu@$(terraform output aws_consul_public_ip) 'tail -f /var/log/cloud-init-output.log'`
    * Consul MGW - `ssh ubuntu@$(terraform output aws_mgw_public_ip) 'tail -f /var/log/cloud-init-output.log'`

    Wait for the server to elect a leader (it can take a few minutes for the EC2 instance to become available). <br>

    ```
    consul_api=$(terraform output aws_consul_public_ip)
    curl -s -v http://{$consul_api}:8500/v1/status/leader
    ```

    In the next assignment you will finish configuring the Consul primary server.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/aws-consul-primary
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Shell
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 14400
- slug: 06-bootstrap-consul-primary
  id: nu6xyinepsdt
  type: challenge
  title: Bootstrap Consul Primary Datacenter
  teaser: Set up Consul for multi-datacenter
  assignment: |-
    In this assignment you will finish configuring the primary server
    and configure tokens and policies for federated clusters to connect to the primary. <br>

    * [Vault CA](https://www.consul.io/docs/connect/ca/vault)
    * [Secure Multi DC](https://learn.hashicorp.com/tutorials/consul/access-control-replication-multiple-datacenters)
    * [Proxy Defaults](https://www.consul.io/docs/agent/config-entries/proxy-defaults)

    The Consul server is now available in the UI. <br>

    The Consul server was initialized with a [master token](https://www.consul.io/docs/security/acl/acl-system#builtin-tokens)
    to facilitate the bootstrap process. <br>

    Log in to Vault and get an admin token to finish setting up the Consul server cluster. <br>

    ```
    vault login -method=userpass username=admin password=admin
    export CONSUL_HTTP_TOKEN=$(vault kv get -field master_token kv/consul)
    ```

    Vault can [create short lived tokens](https://www.vaultproject.io/docs/secrets/consul) for Consul access.
    Configure that now so we can provide least privilege to operators. <br>

    ```
    AWS_CONSUL_IP=$(terraform output -state /root/terraform/aws-consul-primary/terraform.tfstate aws_consul_public_ip)
    consul acl policy create -name operator -rules @/root/policies/consul/operator.hcl
    vault secrets enable consul
    vault write consul/config/access \
        address=http://${AWS_CONSUL_IP}:8500 \
        token=$(consul acl token create -description 'vault mgmt' -policy-name=global-management -format=json | jq -r '.SecretID')
    vault write consul/roles/operator policies=operator ttl=30m
    ```

    You can now request a short lived token to make administrative changes to the cluster. <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/operator)
    consul acl token read -self
    ```

    Add a policy for the Consul agents. <br>

    ```
    consul acl policy create -name agent -rules @/root/policies/consul/agent.hcl
    vault write consul/roles/agent policies=agent
    ```

    Add a policy for the mesh gateway so this token is dynamically managed by Vault for Consul.
    You may need to wait a minute or so for this to be picked up by the MGW process.

    ```
    consul acl policy create -name mesh-gateway -rules @/root/policies/consul/mesh-gateway.hcl
    vault write consul/roles/mgw policies=mesh-gateway
    sleep 60
    ```
    You can monitor the MGW with the below commands. <br>

    * Consul MGW - `ssh ubuntu@$(terraform output aws_mgw_public_ip) 'journalctl -u consul -f'`

    Check out the Consul Auto Config configuration on the Consul Server and an example Consul Client.  The `node_arn` will allow us to get a node identity signed by Vault.

    Start with the server. <br>

    ```
    ssh ubuntu@$(terraform output aws_consul_public_ip) 'cat /etc/consul.d/server.json' | jq .auto_config
    ```

    Check the MGW client agent config. <br>

    ```
    ssh ubuntu@$(terraform output aws_mgw_public_ip) 'cat /etc/consul.d/auto.json' | jq
    ```

    Check the MGW client agent node.

    ```
    ssh ubuntu@$(terraform output aws_mgw_public_ip) 'curl -s localhost:8500/v1/agent/self' | jq .Config.NodeName
    ```

    Now review the issued intro token and check that it was signed by Vault. The token will have a 30 min expiry.

    ```
    curl -s $VAULT_ADDR/v1/identity/oidc/.well-known/keys  | jq
    ssh ubuntu@$(terraform output aws_mgw_public_ip) 'cat /etc/consul.d/token' | jwt
    ```

    The auto config secrets will be stored in the `data_dir`. These will be auto rotated by the Consul client. <br>

    Check the ACL secrets. <br>

    ```
    ssh ubuntu@$(terraform output aws_mgw_public_ip) 'sudo cat /opt/consul/data/auto-config.json' | jq .Config.ACL
    ```

    Check the Gossip Key. <br>

    ```
    ssh ubuntu@$(terraform output aws_mgw_public_ip) 'sudo cat /opt/consul/data/auto-config.json' | jq .Config.Gossip
    ```

    Next, tie the token in Vault to the replication policy so we can establish trust with federated clusters. <br>

    ```
    consul acl policy create -name replication -rules @/root/policies/consul/replication.hcl
    vault write consul/roles/replication policies=replication
    ```

    Add an additionally policy for Vault SD for Consul.

    ```
    consul acl policy create -name vault -rules @/root/policies/consul/vault.hcl
    vault write consul/roles/vault policies=vault
    ```

    Check the CA infrastructure for the primary. Notice the `pri-` prefix for primary DC. <br>

    ```
    curl -s ${CONSUL_HTTP_ADDR}/v1/connect/ca/roots | jq '.Roots'
    curl -s ${CONSUL_HTTP_ADDR}/v1/connect/ca/roots | jq -r '.Roots[0].RootCert' | openssl x509 -text -noout
    ```

    Last, apply defaults for the service mesh that you'll leverage in the last few assignments. <br>

    ```
    consul config write proxy-defaults.hcl
    ```

    In the next few assignments you will connect secondary Consul datacenters to this cluster.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/aws-consul-primary
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  difficulty: basic
  timelimit: 300
- slug: 07-create-namespaces-and-policies
  id: kybktk3g50me
  type: challenge
  title: Create Namespaces & Policies
  teaser: Configure multi-tenancy
  assignment: |-
    In this assignment you will create namespaces in Consul for development groups. <br>
    Get an operator token. <br>
    ```
    vault login -method=userpass username=admin password=admin
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/operator)
    ```
    Create the namespaces. <br>
    ```
    consul acl policy create -name "cross-namespace-policy-sd" -description "cross-namespace service discovery" -rules @cross-namespace-sd.hcl
    consul namespace update -name default -default-policy-name=cross-namespace-policy-sd
    consul namespace write payments-namespace.hcl
    consul namespace write product-namespace.hcl
    consul namespace write frontend-namespace.hcl
    ```
    Create the developer policies and link them to vault roles. <br>
    ```
    consul acl policy create -name "payments-developer-policy" -description "payments devloper" -rules @payments-developer.hcl
    consul acl policy create -name "product-developer-policy" -description  "product developer" -rules @product-developer.hcl
    consul acl policy create -name "frontend-developer-policy" -description "frontend developer" -rules @frontend-developer.hcl
    vault write consul/roles/payments-developer policies=payments-developer-policy ttl=30m
    vault write consul/roles/product-developer  policies=product-developer-policy ttl=30m
    vault write consul/roles/frontend-developer policies=frontend-developer-policy ttl=30m
    ```
    Dev teams will deploy application workloads to the above namespaces in future assignments.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Policies
    type: code
    hostname: cloud-client
    path: /root/policies
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  difficulty: basic
  timelimit: 300
- slug: 08-provision-azure-consul-secondary-datacenter
  id: htsppowub1zn
  type: challenge
  title: Provision Azure Consul Secondary Datacenter
  teaser: Run Consul in Azure
  assignment: |-
    In this assignment you will bootstrap the Azure secondary Cluster, validate the health of the server and its connection to the primary.

    Inspect the initialization scripts in the CLI or UI.

    ```
    cat scripts/azure_consul_server.sh
    cat scripts/azure_mesh_gateway.sh
    ```

    Inspect the Terraform and provision the servers.

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    You can monitor the init scripts with the below commands. <br>

    * Consul Server - `ssh ubuntu@$(terraform output azure_consul_public_ip) 'tail -f /var/log/cloud-init-output.log'`
    * Consul MGW    - `ssh ubuntu@$(terraform output azure_mgw_public_ip) 'tail -f /var/log/cloud-init-output.log'`


    Wait for the server to elect a leader (it can take a few minutes for the Azure compute instance to become available).
    Replication with the primary will be enabled. <br>

    ```
    consul_api=$(terraform output azure_consul_public_ip)
    curl -s -v http://{$consul_api}:8500/v1/status/leader
    curl -s -v http://{$consul_api}:8500/v1/acl/replication | jq
    ```

    Check the CA infrastructure for the secondary. Notice the `sec-` prefix for secondary DC. <br>

    ```
    consul_api=$(terraform output azure_consul_public_ip)
    curl -s http://${consul_api}:8500/v1/connect/ca/roots | jq '.Roots'
    curl -s http://${consul_api}:8500/v1/connect/ca/roots | jq -r '.Roots[0].IntermediateCerts[0]' | openssl x509 -text -noout
    ```

    In the next assignment you will configure the remaining secondary cluster.
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/azure-consul-secondary
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 300
- slug: 09-provision-gcp-consul-secondary-datacenter
  id: hm95rulre8ei
  type: challenge
  title: Provision GCP Consul Secondary Datacenter
  teaser: Run Consul in GKE
  assignment: |-
    In this assignment you will bootstrap the GCP secondary Cluster, validate the health of the server and its connection to the primary. <br>

    In this environment, Consul on GCP runs entirely in GKE K8s. The Consul helm chart easily supports running Consul in K8s for both Consul server agents, and Consul client agents. <br>

    You can read the following resources for more information on running Consul in K8s: <br>
      * https://www.consul.io/docs/k8s
      * https://www.consul.io/docs/k8s/installation/multi-cluster/kubernetes

    Inspect the Terraform and provision K8s shared services cluster. <br>

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    Check the worker nodes are available for the cluster. <br>

    ```
    gcloud container clusters get-credentials $(terraform output gcp_gke_cluster_shared_name) --region us-central1-a
    kubectl config rename-context $(kubectl config current-context) shared
    kubectl config use-context shared
    kubectl get nodes
    ```

    Now that K8s is ready, you create the K8s federation secret.

    ```
    vault login -method=userpass username=admin password=admin
    aws_mgw=$(terraform output -state /root/terraform/aws-consul-primary/terraform.tfstate aws_mgw_public_ip)
    server_json=$(jq -n --arg mgw "$aws_mgw" '{primary_datacenter: "aws-us-east-1",primary_gateways:["\($mgw):443"]}')
    cat <<EOF | kubectl apply -f -
    {
    "apiVersion": "v1",
    "kind": "Secret",
      "data": {
        "caCert": "$(vault read -field certificate pki/cert/ca | base64 -w 0)",
        "caKey": "$(vault kv get -field private_key kv/pki | base64 -w 0)",
        "gossipEncryptionKey": "$(vault kv get -field gossip_key kv/consul | base64 -w 0)",
        "replicationToken": "$(vault read -field token consul/creds/replication | base64 -w 0)",
        "serverConfigJSON": "$(echo $server_json | base64 -w 0)"
        },
        "metadata": {
            "name": "consul-federation",
            "namespace": "default"
        }
    }
    EOF
    ```

    Next, deploy the Consul servers. You can review the configuration in the code editor.

    ```
    kubectl create secret generic consul-ent-license --from-literal="key=$(cat /etc/consul.hclic)"
    helm install hashicorp hashicorp/consul -f /root/helm/gke-consul-values.yaml --debug --wait --version 0.33.0
    ```

    Check that all three clusters are federated.

    ```
    consul members -wan
    ```

    In the next assignment you will connect Consul secondary components to the server clusters in AWS & Azure.
  tabs:
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/gcp-consul-secondary
  - title: Helm
    type: code
    hostname: cloud-client
    path: /root/helm
  difficulty: basic
  timelimit: 300
- slug: 10-provision-consul-esms
  id: jftjytsuegl5
  type: challenge
  title: Provision Consul ESMs
  teaser: Create health checks for external services
  assignment: |-
    In this assignment you will provision Consul External Services Monitors (ESMs) to health check services that do not run Consul agents. <br>

    Cloud managed services are common targets for external services monitoring. <br>

    You can read more about Consul ESM:
      * https://learn.hashicorp.com/tutorials/consul/service-registration-external-services
      * https://github.com/hashicorp/consul-esm
      * https://www.hashicorp.com/resources/bloomberg-s-consul-story-to-20-000-nodes-and-beyond

    Get creds for this operation. <br>

    ```
    vault login -method=userpass username=admin password=admin
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/operator)
    ```

    Create the ESM policy. <br>

    ```
    consul acl policy create -name consul-esm -rules @/root/policies/consul/consul-esm.hcl
    vault write consul/roles/esm policies=consul-esm
    ```

    Inspect the Terraform code and provision the external monitoring.

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    You can monitor the init scripts with the below commands. <br>
    * AWS ESM - `ssh ubuntu@$(terraform output aws_esm_public_ip) 'tail -f /var/log/cloud-init-output.log'`
    * Azure ESM - `ssh ubuntu@$(terraform output azure_esm_public_ip) 'tail -f /var/log/cloud-init-output.log'`


    ESM services are now available in your Consul datacenters. <br>

    ```
    consul catalog services -datacenter=aws-us-east-1
    consul catalog services -datacenter=azure-west-us-2
    ```

    In the next assignments you will provision cloud managed services and configure them for Consul ESM monitoring.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/esm
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  difficulty: basic
  timelimit: 300
- slug: 11-provision-cache-services
  id: tdwcriazb1xk
  type: challenge
  title: Provision Cache Services
  teaser: Deploy managed Cache instances
  assignment: |-
    In this assignment you will provision AWS ElastiCache instances to be consumed by the application.
    The ElastiCache instances are Redis datastores. <br>

    We can use Terraform to configure Consul with the [Consul Provider](https://registry.terraform.io/providers/hashicorp/consul/latest/docs).

    Inspect the Terraform, retrieve an operator token, and provision the Cache instances. <br>

    ```
    vault login -method=userpass username=admin password=admin
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/operator)
    terraform plan
    terraform apply -auto-approve
    ```

    The managed Cache instance will now be available in the catalog.

    ```
    curl -s "${CONSUL_HTTP_ADDR}/v1/health/service/redis?passing=true" | jq
    ```

    You will use the managed Redis instance to process payments in future assignments.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/cache-services
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  difficulty: basic
  timelimit: 300
- slug: 12-provision-database-services
  id: rwxcrn3y0fa7
  type: challenge
  title: Provision Database Services
  teaser: Deploy managed DB instances
  assignment: |-
    In this assignment you will provision Azure Database Postgres instance to be consumed by the application. <br>

    We can use Terraform to configure Consul with the [Consul Provider](https://registry.terraform.io/providers/hashicorp/consul/latest/docs).

    Inspect the Terraform, retrieve an operator token, and provision the Database instances. <br>

    ```
    vault login -method=userpass username=admin password=admin
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/operator)
    terraform plan
    terraform apply -auto-approve
    ```

    The managed database instance will now be available in the catalog.

    ```
    curl -s "${CONSUL_HTTP_ADDR}/v1/health/service/postgres?dc=azure-west-us-2&passing=true" | jq
    ```

    You will use the managed Postgres instance to query available products in future assignments.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/database-services
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  difficulty: basic
  timelimit: 300
- slug: 13-provision-consul-terminating-gateways
  id: onikoufhplbd
  type: challenge
  title: Provision Consul Terminating Gateways
  teaser: Configure egress traffic for external services
  assignment: |-
    In this assignment you will provision Consul Terminating Gateways(TGW).
    You can read [the docs](https://www.consul.io/docs/connect/gateways/terminating-gateway) for more information on how TGW works. <br>

    Get credentials. <br>

    ```
    vault login -method=userpass username=admin password=admin
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/operator)
    ```

    Create the policies for the TGWs.

    ```
    consul acl policy create -name aws-terminating-gateway -rules @/root/policies/consul/aws-tgw.hcl
    vault write consul/roles/aws-tgw policies=aws-terminating-gateway
    consul acl policy create -name azure-terminating-gateway -rules @/root/policies/consul/azure-tgw.hcl
    vault write consul/roles/azure-tgw policies=azure-terminating-gateway
    ```

    Inspect the Terraform, retrieve an operator token, and provision the TGWs. <br>

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    You can monitor provisioning with the below commands: <br>

    * AWS TGW - `ssh ubuntu@$(terraform output aws_tgw_public_ip) 'tail -f /var/log/cloud-init-output.log'`
    * Azure TGW - `ssh ubuntu@$(terraform output azure_tgw_public_ip) 'tail -f /var/log/cloud-init-output.log'`

    TGW services are now running. <br>

    ```
    consul catalog services -datacenter=aws-us-east-1
    consul catalog services -datacenter=azure-west-us-2
    ```

    In future assignments you will route traffic to Vault, Redis, and Postgres leveraging the TGW in AWS & Azure.
  tabs:
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/tgw
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  difficulty: basic
  timelimit: 300
- slug: 14-provision-nomad-scheduler-services
  id: bwq0s9w3zhx3
  type: challenge
  title: Provision Nomad Scheduler Services
  teaser: Deploy Nomad workload infrastructure
  assignment: |-
    In this assignment you will provision Nomad to run payments workloads in AWS.
    You can read the docs for more information about the Nomad integrations used in this lab environment: <br>

    * [Introduction](https://www.nomadproject.io/intro)
    * [Nomad & Vault](https://www.nomadproject.io/docs/integrations/vault-integration)
    * [Nomad & Consul](https://www.nomadproject.io/docs/integrations/consul-integration)
    * [Nomad & Consul Connect](https://www.nomadproject.io/docs/integrations/consul-connect)

    Inspect the Terraform and provision Nomad servers and clients. <br>

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    You can monitor provisioning with the below command: <br>

    ```
    ssh ubuntu@$(terraform output aws_nomad_server_public_ip) 'tail -f /var/log/cloud-init-output.log'
    ```

    You can access the Nomad UI at the below URL: <br>

    ```
    echo http://$(terraform output aws_nomad_server_public_ip):4646
    ```

    Nomad services are now running in AWS. <br>

    ```
    consul catalog services -datacenter=aws-us-east-1
    ```

    You will schedule workloads on this nomad cluster in a later assignment.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/nomad-scheduler-services
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  difficulty: basic
  timelimit: 300
- slug: 15-provision-k8s-scheduler-services
  id: jxpwtqcessex
  type: challenge
  title: Provision K8s Scheduler Services
  teaser: Deploy K8s workload infrastructure
  assignment: |-
    In this assignment you will provision K8s workload clusters in GCP.
    These clusters will connect to the shared services Consul GKE cluster in GCP. <br>

    You can read the docs for more information on these patterns: <br>

    * [Overview](https://www.consul.io/docs/k8s)
    * [Multi-Cluster](https://www.consul.io/docs/k8s/installation/multi-cluster)

    Inspect the Terraform and provision the K8s clusters. <br>

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    Next, configure the two provisioned workload K8s clusters and connect them to the shared svcs cluster.


    First, deploy the React cluster. <br>

    ```
    vault login -method=userpass username=admin password=admin
    gcloud container clusters get-credentials $(terraform output -state /root/terraform/k8s-scheduler-services/terraform.tfstate gcp_gke_cluster_react_name) --region us-central1-a
    kubectl config rename-context $(kubectl config current-context) react
    kubectl config use-context react
    kubectl create secret generic hashicorp-consul-ca-cert --from-literal="tls.crt=$(vault read -field certificate pki/cert/ca)"
    kubectl create secret generic hashicorp-consul-gossip-key --from-literal="key=$(vault kv get -field=gossip_key kv/consul)"
    kubectl create secret generic bootstrap-token --from-literal="token=$(vault read -field token consul/creds/operator)"
    helm install consul hashicorp/consul --set externalServers.k8sAuthMethodHost="https://$(terraform output gcp_gke_cluster_react_endpoint)" -f /root/helm/react-consul-values.yaml --debug --wait --timeout 10m --version 0.33.0
    helm list -a
    ```

    Last, deploy the Graphql cluster. <br>

    ```
    vault login -method=userpass username=admin password=admin
    gcloud container clusters get-credentials $(terraform output -state /root/terraform/k8s-scheduler-services/terraform.tfstate gcp_gke_cluster_graphql_name) --region us-central1-a
    kubectl config rename-context $(kubectl config current-context) graphql
    kubectl config use-context graphql
    kubectl create secret generic hashicorp-consul-ca-cert --from-literal="tls.crt=$(vault read -field certificate pki/cert/ca)"
    kubectl create secret generic hashicorp-consul-gossip-key --from-literal="key=$(vault kv get -field=gossip_key kv/consul)"
    kubectl create secret generic bootstrap-token --from-literal="token=$(vault read -field token consul/creds/operator)"
    helm install consul hashicorp/consul --set externalServers.k8sAuthMethodHost="https://$(terraform output gcp_gke_cluster_graphql_endpoint)" -f /root/helm/graphql-consul-values.yaml --debug --wait --timeout 10m --version 0.33.0
    helm list -a
    ```

    The K8s nodes are now registered in the Consul catalog.

    ```
    consul catalog nodes -datacenter gcp-us-central-1
    ```

    In later assignments you will use these clusters to run frontend application workloads.
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/k8s-scheduler-services
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Helm
    type: code
    hostname: cloud-client
    path: /root/helm
  difficulty: basic
  timelimit: 500
- slug: 16-configure-intentions
  id: 1pfkcfoq8ysz
  type: challenge
  title: Configure Intentions
  teaser: Allow selective mTLS between workloads
  assignment: |-
    In this assignment you will log in as each persona and configure intentions for self-service access.

    Allow least privilege access to shared services as an infrastructure operator.

    ```
    vault login -method=userpass username=admin password=admin
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/operator)
    consul intention create -allow '*/*' 'default/vault'
    consul intention create -allow 'default/payments-api' 'default/redis'
    consul intention create -allow 'frontend/public-api' 'default/payments-api'
    consul intention create -allow 'product/*' 'default/postgres'
    ```

    As a product developer grant access to frontend team APIs to connect to product services.

    ```
    vault login -method=userpass username=product-developer password=product
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/product-developer)
    consul intention create -allow 'frontend/public-api' 'product/product-api'
    ```

    As a frontend developer, you will manage intentions with CRDs in a later exercise.

    In later assignments you will use these clusters to run frontend application workloads.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/policies
  difficulty: basic
  timelimit: 300
- slug: 17-deploy-payments-applications
  id: cruyignfbudn
  type: challenge
  title: Deploy Payments Tier
  teaser: Run Payments workloads
  assignment: |-
    In this assignment you will schedule the payments-api with Nomad.

    Run the nomad job.

    ```
    nomad run payments-api.hcl
    ```

    Payments API services should now be running in the cluster.

    ```
    curl -s ${CONSUL_HTTP_ADDR}/v1/health/service/payments-api?passing=true | jq
    ```

    In future assignments you will use these APIs to process payments.
  tabs:
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/nomad-scheduler-services
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Apps
    type: code
    hostname: cloud-client
    path: /root/apps/nomad
  difficulty: basic
  timelimit: 300
- slug: 18-deploy-product-applications
  id: eav10xzem4qs
  type: challenge
  title: Deploy Product Tier
  teaser: Run Product workloads
  assignment: |-
    In this assignment you will deploy product applications in Azure.

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    You can monitor provisioning with the below command: <br>

    ```
    ssh ubuntu@$(terraform output azure_product_api_public_ip) 'tail -f /var/log/cloud-init-output.log'
    ```

    Product API services should now be running in the cluster.

    ```
    consul catalog services -datacenter azure-west-us-2 -namespace=product
    ```

    In future assignments you will use these APIs to process product queries.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/product-applications
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  difficulty: basic
  timelimit: 300
- slug: 19-deploy-frontend-applications
  id: z76koxyd2ptx
  type: challenge
  title: Deploy Frontend Tier
  teaser: Run Frontend workloads
  assignment: |-
    In this assignment you will deploy frontend applications to the GKE clusters. <br>

    Review the k8s deployment specs in the code editor. <br>


    Deploy the Public APIs. This API will be exposed via a Consul ingress gateway. <br>

    ```
    kubectl config use-context graphql
    kubectl apply -f k8s/public-api
    ```

    Deploy the React Frontend. This frontend will be exposed via an ingress controller with a Connect transparent proxy. <br>

    ```
    kubectl config use-context react
    helm install nginx-ingress -f /root/helm/nginx-ingress.yml ingress-nginx/ingress-nginx  --debug --wait
    sleep 10
    kubectl apply -f k8s/web
    ```

    Frontend services are now available.

    ```
    consul catalog services -datacenter gcp-us-central-1 -namespace=frontend
    ```

    In future assignments you will use these web interfaces and APIs to send traffic to backend application components.
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Helm
    type: code
    hostname: cloud-client
    path: /root/helm
  - title: Apps
    type: code
    hostname: cloud-client
    path: /root/apps
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  difficulty: basic
  timelimit: 300
- slug: 20-review-application-deployment
  id: yfp4xyirkhja
  type: challenge
  title: Review Application Deployment
  teaser: Validate workload configuration
  assignment: |-
    In this assignment you will validate the product DB and access to the payment queue. <br>

    Check the DB for available products. <br>

    ```
    export PGPASSWORD=$(terraform output -state /root/terraform/database-services/terraform.tfstate postgres_password)
    psql -U postgres \
      -d postgres \
      -h $(terraform output -state /root/terraform/database-services/terraform.tfstate postgres_fqdn) \
      -c 'SELECT * FROM coffees' \
      -a
    ```

    Check the queue for payments (there will be zero keys). <br>

    ```
    ssh -i ~/.ssh/id_rsa ubuntu@$(terraform output -state /root/terraform/infra/terraform.tfstate aws_bastion_ip) \
      "redis-cli -h \
      $(terraform output -state /root/terraform/cache-services/terraform.tfstate -json aws_elasticache_cache_nodes | jq -r .[0].address) -p 6379 keys '*'"
    ```

    In the next few assignments you will retrieve products and send payments.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  difficulty: basic
  timelimit: 300
- slug: 21-test-application-deployment
  id: nhbawzeqgdha
  type: challenge
  title: Test Application Deployment
  teaser: Put it all together
  assignment: |-
    In this assignment you will test the application. <br>

    The UI is available on the react cluster. <br>

    ```
    kubectl config use-context react
    echo "http://$(kubectl get svc nginx-ingress-controller -o json | jq -r .status.loadBalancer.ingress[0].ip)"
    ```

    Monitor the cache for incoming payments in one window. <br>

    ```
    ssh -i ~/.ssh/id_rsa ubuntu@$(terraform output -state /root/terraform/infra/terraform.tfstate aws_bastion_ip) \
      "redis-cli -h \
      $(terraform output -state /root/terraform/cache-services/terraform.tfstate -json aws_elasticache_cache_nodes | jq -r .[0].address) -p 6379 MONITOR"
    ```

    In the other window send traffic to the HashiCups public APIs. <br>

    ```
    kubectl config use-context graphql
    endpoint=$(kubectl get svc consul-graphql-graphql-ingress-gateway -o json | jq -r .status.loadBalancer.ingress[0].ip)
    ```

    Try the Product API. <br>

    ```
    curl -s -v http://${endpoint}:8080/api \
      -H 'Accept-Encoding: gzip, deflate, br' \
      -H 'Content-Type: application/json' \
      -H 'Accept: application/json' \
      -H 'Connection: keep-alive' \
      -H 'DNT: 1' \
      --data-binary '{"query":"{\n  coffees{id,name,price}\n}"}' \
      --compressed | jq
    ```

    Try the payment API. <br>

    #payments

    ```
    curl -s -v http://${endpoint}:8080/api \
      -H 'Accept-Encoding: gzip, deflate, br' \
      -H 'Content-Type: application/json' \
      -H 'Accept: application/json' \
      -H 'Connection: keep-alive' \
      -H 'DNT: 1' \
      --data-binary '{"query":"mutation{ pay(details:{ name: \"nic\", type: \"mastercard\", number: \"1234123-0123123\", expiry:\"10/02\", cv2: 1231, amount: 12.23 }){id, card_plaintext, card_ciphertext, message } }"}' \
      --compressed | jq
    ```

    You just connected the HashiCups app across three clouds!
  tabs:
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Web App
    type: service
    hostname: cloud-client
    path: /
    port: 8080
  difficulty: basic
  timelimit: 300
- slug: 22-observe-application-deployment
  id: iriwn61yrqyf
  type: challenge
  title: Observe Application Deployment
  teaser: Collect metrics and traces across clouds.
  assignment: |-
    In this assignment you will view traces in the Jaeger UI. <br>

    Simulate application traffic. <br>

    ```
    kubectl config use-context graphql
    endpoint=$(kubectl get svc consul-graphql-graphql-ingress-gateway -o json | jq -r .status.loadBalancer.ingress[0].ip)
    curl -s -v http://${endpoint}:8080/api \
      -H 'Accept-Encoding: gzip, deflate, br' \
      -H 'Content-Type: application/json' \
      -H 'Accept: application/json' \
      -H 'Connection: keep-alive' \
      -H 'DNT: 1' \
      --data-binary '{"query":"{\n  coffees{id,name,price}\n}"}' \
      --compressed | jq
    curl -s -v http://${endpoint}:8080/api \
      -H 'Accept-Encoding: gzip, deflate, br' \
      -H 'Content-Type: application/json' \
      -H 'Accept: application/json' \
      -H 'Connection: keep-alive' \
      -H 'DNT: 1' \
      --data-binary '{"query":"mutation{ pay(details:{ name: \"nic\", type: \"mastercard\", number: \"1234123-0123123\", expiry:\"10/02\", cv2: 1231, amount: 12.23 }){id, card_plaintext, card_ciphertext, message } }"}' \
      --compressed | jq
    ```

    Review the traces in the Jaeger UI for each cloud.
  tabs:
  - title: Jaeger
    type: service
    hostname: cloud-client
    path: /search?service=public-api
    port: 16686
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Shell
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Lab Architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/add-consul-multi-cloud/instruqt-tracks/multi-cloud-service-networking-with-consul/assets/diagrams/diagrams.html
  - title: Web App
    type: service
    hostname: cloud-client
    path: /
    port: 8080
  difficulty: basic
  timelimit: 300
checksum: "17308107734533554434"
