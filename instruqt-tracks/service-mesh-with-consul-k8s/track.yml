slug: service-mesh-with-consul-k8s
id: ygz7z7sj3nyn
type: track
title: 'DEPRECATED: Service mesh with Consul K8s'
teaser: 'See link for new track: https://play.instruqt.com/hashicorp/tracks/consul-life-of-a-developer'
description: |-
  In this track you will use Consul Connect's first-class support on K8s
  for service mesh.

  You'll work through scaling, monitoring, and tracing applications. To complete the track you, will gain experience with the advanced L7 traffic patterns.
icon: https://storage.googleapis.com/instruqt-frontend/assets/hashicorp/tracks/consul.png
tags: []
owner: hashicorp
developers:
- lance@hashicorp.com
private: true
published: true
show_timer: true
skipping_enabled: true
challenges:
- slug: 01-deploy-connect
  id: iyea2sohruxq
  type: challenge
  title: Deploy Connect
  teaser: Your first Consul cluster on K8s
  assignment: |-
    First, use Helm with a custom values file to deploy Consul into the K8s cluster and enable Connect. <br>

    The helm chart has been cloned from GitHub and placed it in your lab environment.
    Click on the `/root/consul-valules.yaml` file to view the Consul deployment configuration in the code editor. <br>

    Navigate to the `_K8s` shell to install Consul. Note, Helm has been pre-initialized in this environment. Copy or type the following command in the shell.<br>

    ```
    helm install -f /root/consul-values.yaml lab hashicorp/consul  --wait --debug
    ```

    The status output should indicate Consul was `DEPLOYED`. <br>


    The Consul UI will become available once the Consul server pods are running. Monitor the progress with the following `kubectl` command. If `lab-consul-server-0` is `running`, then you should be able to access the Consul UI <br>

    ```
    kubectl get pods
    ```

    <br>

    Now that you have successfully deployed Consul on K8s, you can deploy some apps! To continue, click the `Check` button.

    **Optional**

    We've included the K8s dashboard for the duration of this lab if you want to compare the workloads using the K8s Dashboard tab.
    You can use the token included in the tabs to authenticate.  <br>

    If you want to see a breakout of the Consul deployment from helm, run the following command. <br>

    ```
    helm status lab
    ```
  notes:
  - type: text
    contents: |-
      Connect can be used with Kubernetes to secure pod communication with other pods and external Kubernetes services.
      The Connect sidecar proxy running Envoy can be automatically injected into pods in your cluster, making configuration for Kubernetes automatic. <br>

      This functionality is provided by the consul-k8s project and can be automatically installed and configured using the official Consul Helm chart.
  - type: text
    contents: |-
      We recommend running Consul on Kubernetes with the same general architecture as running it anywhere else.
      There are some benefits Kubernetes can provide that eases operating a Consul cluster and Connect mesh that we will explore.
      The standard production [deployment guide](https://learn.hashicorp.com/consul/datacenter-deploy/deployment-guide?utm_source=instruqt&utm_medium=k8s-track&utm_term=dg) is still an important read even if running Consul within Kubernetes. <br>

      We use a lightweight distro of K8s for this lab called [k3s](https://k3s.io/).
      Your environment consists of a single node cluster of K8s.
      We will expose services over [NodePorts](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport) in our lab for simplicity.
      This includes access to the Consul API on our K8s workstation.
  - type: image
    url: https://d33wubrfki0l68.cloudfront.net/949e085caf846f7e512f420bcbd0d1a2935e27bb/4c93c/static/img/k8s-consul-simple.png
  tabs:
  - title: Helm Config
    type: code
    hostname: kubernetes
    path: /root/consul-values.yaml
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: K8s Dashboard Token
    type: code
    hostname: kubernetes
    path: /root/dashboard-token.txt
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s Dashboard
    type: service
    hostname: kubernetes
    path: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
    port: 8001
  difficulty: basic
  timelimit: 600
- slug: 02-deploy-application
  id: 71ekxt1ehjg5
  type: challenge
  title: Deploy the Application
  teaser: Bring our application to the world.
  assignment: |-
    Start by deploying the default configuration for the mesh. <br>

    ```
    kubectl apply -f proxy-defaults.yaml
    ```

    Before deploying your apps, investigate your config for the Emojify app in the code editor called `Emojify-Config`.
    Select any of the YAML files, you'll notice the service has the following annotations.

    ```
    consul.hashicorp.com/connect-inject
    consul.hashicorp.com/connect-service-upstreams
    ```

    If you view any of the other services, they will include the same annotations. <br>

    These annotations tell Consul that the pod should get an Envoy sidecar proxy to route traffic
    and listeners for the requested service on the specified upstream port.
    In the helm chart, you set  `connectInject` to `false`, so now you need to explicitly add the above annotations to deploy a sidecar proxy with your application.<br>

    ```
    connectInject:
      default: false
    ```

    Now, you can deploy your applications. Navigate back to the `_K8s` shell and execute the following commands. <br>

    ```
    kubectl apply -f emojify/ingress.yml
    kubectl apply -f emojify/website_sa.yml
    kubectl apply -f emojify/website_v1.yml
    kubectl apply -f emojify/api.yml
    kubectl apply -f emojify/cache.yml
    kubectl apply -f emojify/facebox.yml
    ```

    Wait for the pods. <br>

    ```
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-ingress -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-website -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-api -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-cache -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-facebox -o name)
    ```

    By default, the Consul cluster has been configured to deny all traffic between applications. A deny all by default configuration is best practice. You can review the intentions managed by K8s CRDs in the Consul UI.

    Now you've deployed your applications and created allow intentions, you should be able to access the website on the `Emojify-Website` tab.
  notes:
  - type: text
    contents: |-
      In this exercise, you will deploy a more complex microservice application called [Emojify](https://github.com/emojify-app). <br>

      The application has five distinct components that provide functionality:

      * Ingress - Nginx container that gives us access to the API & Website.
      * Website - Serves static content for the Emojify website.
      * API - Provides API to machine learning backend.
      * Cache - Cache layer for API.
      * Facebox - Provides machine learning for detecting and identifies faces in photos.

      In this exercise you'll also explore how these services can be easily connected, monitored, and scaled with Connect.
  tabs:
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/dc1/intentions
    port: 30085
  - title: K8s Dashboard
    type: service
    hostname: kubernetes
    path: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
    port: 8001
  - title: Emojify - Config
    type: code
    hostname: kubernetes
    path: /root/emojify
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Emojify - Website
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  - title: K8s Dashboard Token
    type: code
    hostname: kubernetes
    path: /root/dashboard-token.txt
  difficulty: basic
  timelimit: 600
- slug: 03-use-the-application
  id: ibdhfvn7xzbo
  type: challenge
  title: Use the Application
  teaser: You get an emoji, and you get an emoji, and you get an emoji.
  assignment: |-
    To use your Emojify app, enter a URL into the website to "emojify" the image. If you don't have an image you'd like to use try the one below of our founders.

    ```
    https://cdn.geekwire.com/wp-content/uploads/2017/10/armon-dadgar-and-mitchell-hashimoto-630x419.jpg
    ```

    Next, in the `Api-Service` tab, view the Consul service definition generated based on your K8s YAML files.
    You can also discover the service definition using `kubectl`. In the `_K8s` shell tab, execute the command below. <br>

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-api -o name) -c envoy-sidecar -- cat  /consul/connect-inject/service.hcl
    ```

    If you have previous Consul experience, you have practice creating service definitions by hand.
    In K8s, all you needed is the following to generate all the necessary configurations.

    ```
    template:
      metadata:
        labels:
          app: emojify-api
        annotations:
          "consul.hashicorp.com/connect-inject": "true"
          "consul.hashicorp.com/connect-service-upstreams": "emojify-facebox:8003,emojify-cache:8005"
    ```

    Now that you've tested your Emojify app, you can move to more advanced use cases!
  notes:
  - type: text
    contents: |-
      Now that your app is working, you can Emojify some faces.
      Feel free to use your colleagues, or our sample. <br>
  tabs:
  - title: Api -  Service
    type: code
    hostname: kubernetes
    path: /tmp/api-service.hcl
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: K8s Dashboard
    type: service
    hostname: kubernetes
    path: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
    port: 8001
  - title: Emojify - Website
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  - title: K8s Dashboard Token
    type: code
    hostname: kubernetes
    path: /root/dashboard-token.txt
  difficulty: basic
  timelimit: 600
- slug: 04-scale-the-app
  id: r2vgop1q59ix
  type: challenge
  title: Scale the Application
  teaser: Handle increased demand
  assignment: |-
    To scale the number of Facebox services providing our ML capabilities, increase the `replicas`. <br>

    In the emojify app specs from earlier, change the number of replicas for the Facebox service to `2`. Note, save the changes by clicking the "save" icon next to the file name.

    ```
    spec:
      replicas: 2
    ```

    Next, use `kubectl` to update the spec.

    ```
    kubectl apply -f emojify/facebox.yml
    ```

    Finally, verify the Consul catalog has two healthy services. You can also do this in the `Consul UI` tab or by using the HTTP API command below in the `_K8s` shell.

    ```
    curl -s localhost:30085/v1/catalog/service/emojify-facebox | jq '[.. |."ServiceAddress"? | select(. != null)]'
    ```

    You should also verify Envoy sidecar proxy is healthy using `kubectl`.

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-api -o name) -c envoy-sidecar -- wget -qO- 127.0.0.1:19000/clusters | grep facebox
    ```

    Nice work! You just leveraged dynamic service discovery inside your service mesh to scale out a workload.
  notes:
  - type: text
    contents: |-
      Now that users have seen the power of Emojify, they are flocking to the application! <br>

      In the next exercise you'll scale your backend services and preserve continuity in the mesh.
  tabs:
  - title: Facebox - Config
    type: code
    hostname: kubernetes
    path: /root/emojify/facebox.yml
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: K8s Dashboard
    type: service
    hostname: kubernetes
    path: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
    port: 8001
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s Dashboard Token
    type: code
    hostname: kubernetes
    path: /root/dashboard-token.txt
  difficulty: basic
  timelimit: 600
- slug: 05-get-metrics
  id: hwu25pwhbljf
  type: challenge
  title: Get Metrics
  teaser: Please observe
  assignment: |-
    Your monitoring stack includes Grafana and Prometheus. Ensure both applications have healthy, running pods using `kubectl`. <br>

    ```
    kubectl get pod --selector=app=prometheus
    kubectl get pod --selector=app.kubernetes.io/name=grafana
    ```

    They were installed via their respective helm charts.

    * [Grafana](https://github.com/helm/charts/tree/master/stable/grafana)
    * [Prometheus](https://github.com/helm/charts/tree/master/stable/prometheus)

    You can log into Grafana in the UI tab with the below credentials:

    * username: `admin`
    * password: `check the password tab`

    There is a sample dashboard built for you that can visualize metrics for your application stack. <br>

    To create the sample dashboard select the `+` in the left side navigation to `Import`.

    ```
    https://grafana.com/grafana/dashboards/13396
    ```

    If you encounter issues, you can follow [the complete instructions](https://grafana.com/docs/reference/export_import/#importing-a-dashboard) from Grafana to import your own dashboard.


    You have a application that can simulate traffic for you. Let's run it now in the `_K8s` shell. <br>

    ```
    kubectl apply -f emojify/traffic.yml
    ```

    Wait a few moments and check your dashboard in the Consul UI tab. Metrics will be streaming in shortly. Nice work!
  notes:
  - type: text
    contents: |-
      In the previous exercise you scaled up a component of your application,
      but how would you have made that determination in the first place? <br>

      In this exercise you'll review detailed telemetry for your microservices.
      This observability is a powerful feature of Consul Connect and gives you real-time insights on our application performance. <br>

      We're spinning up a monitoring stack for you.
      Please be patient. This should take 1-2 minutes.
  tabs:
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/dc1/services/emojify-api/topology
    port: 30085
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Grafana - UI
    type: service
    hostname: kubernetes
    port: 30030
  - title: Grafana - Password
    type: code
    hostname: kubernetes
    path: /tmp/grafana-pass.txt
  - title: Prometheus - UI
    type: service
    hostname: kubernetes
    path: /targets#job-kubernetes-pods
    port: 30090
  - title: K8s Dashboard
    type: service
    hostname: kubernetes
    path: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
    port: 8001
  - title: K8s Dashboard Token
    type: code
    hostname: kubernetes
    path: /root/dashboard-token.txt
  difficulty: basic
  timelimit: 600
- slug: 06-ship-a-new-feature
  id: w95srj56cyzm
  type: challenge
  title: Ship a New Feature
  teaser: Selectively bring users to a new version of the application.
  assignment: |-
    In this exercise, you will use Connect's traffic splitting feature for an a/b deployment. <br>

    Notice, you have a few more files in our `emojify` folder.
    The new service will allow users to optionally purchase their photograph after its emojfied. <br>

    You'll notice in the deployment spec for the `website`, the Consul definition called `consul.hashicorp.com/service-tags`.
    This metadata will allow you to send traffic to either `v1` or `v2` of the website based on certain conditions. <br>

    Start by deploying Connect's L7 routing config for this service. Each of these configurations do a few things. <br>

    * Resolver - Creates a subsets of the website service. In this example, you will use the service tags.

    ```
    cat emojify/resolver.yaml
    ```

    * Splitter - Contains your traffic shaping rules. Since you're pretty confident in your app,
    you'll send 90% of the traffic to the new service.

    ```
    cat emojify/splitter.yaml
    ```

    * Router - Allows you to do some version specific testing by supplying query params.

    ```
    cat emojify/router.yaml
    ```

    Next, apply the routing config. <br>

    ```
    kubectl apply -f emojify/resolver.yaml
    kubectl apply -f emojify/splitter.yaml
    kubectl apply -f emojify/router.yaml
    ```

    With your new routing rules in place, go ahead and push the new version of the website with the payment service.

    ```
    kubectl  apply -f emojify/website_v2.yml
    kubectl  apply -f emojify/payments.yml
    sleep 5
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-payments -o name)
    ```

    Once your pods are healthy, you can start our testing.
    Check Consul for the `Website` service and look at the `ServiceTags` value.
    We should see two tagged versions of the application using the HTTP API. <br>

    ```
    curl -s http://127.0.0.1:8500/v1/catalog/service/emojify-website | jq
    ```

    Use that header value you set up earlier to look at our different versions of the website config.
    Pay attention to the `PAYMENT_ENABLED` value. <br>

    ```
    curl localhost:30000/config/env.js?x-version=1
    curl localhost:30000/config/env.js?x-version=2
    ```

    Now, refresh the app a few times and look for `EmojifyEnterprise` in the top left banner.
    This is the v2 version of our website. <br>

    You can use the image from earlier, and you will have the addiitonal option to purchase it. <br>

    ```
    https://cdn.geekwire.com/wp-content/uploads/2017/10/armon-dadgar-and-mitchell-hashimoto-630x419.jpg
    ```

    Emojify another image and you will see a new option to purchase the image.
    Fill in some dummy details and try it now. <br>

    It looks like you don't have enough funds (don't worry, this happens every time), better call your bank! <br>

    Notice that Envoy is now grouping clusters by your subsets.

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-ingress -o name) -c envoy-sidecar -- wget -qO- 127.0.0.1:19000/clusters  | grep emojify-website
    ```

    You can also see Envoy has updated your route definitions for your debug params and traffic weights. <br>

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-ingress -o name) -c envoy-sidecar -- wget -qO- 127.0.0.1:19000/config_dump | jq '[.. |."routes"? | select(. != null)][-1]'
    ```

    Finally, you can shift all the traffic to the v2 version of the website now that your new feature is working properly.

    ```
    cat <<EOF | kubectl apply -f -
    apiVersion: consul.hashicorp.com/v1alpha1
    kind: ServiceSplitter
    metadata:
      name: emojify-website
    spec:
      splits:
        - weight: 0
          serviceSubset: 'v1'
        - weight: 100
          serviceSubset: 'v2'
    EOF
    ```

    You can additionally verify Consul has propagated this change to Envoy by running the above command again. <br>

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-ingress -o name) -c envoy-sidecar -- wget -qO- 127.0.0.1:19000/config_dump | jq '[.. |."routes"? | select(. != null)][-1]'
    ```

    Nice work! you just used traffic splitting to safely move users to a new version of your application!
  notes:
  - type: text
    contents: |-
      Now that you've scaled the app and used some metrics to determine it's stable, you can ship a new feature.

      In the next exercise, you'll use Connect's L7 routing capabilities to a/b test your application before you roll out to all our users.
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Emojify - Config
    type: code
    hostname: kubernetes
    path: /root/emojify
  - title: Emojify - A/B
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s Dashboard
    type: service
    hostname: kubernetes
    path: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
    port: 8001
  - title: K8s Dashboard Token
    type: code
    hostname: kubernetes
    path: /root/dashboard-token.txt
  difficulty: basic
  timelimit: 900
- slug: 07-tracing
  id: 8cnsp3ybcrlc
  type: challenge
  title: Tracing Distributed Errors
  teaser: Down the rabbit hole we go.
  assignment: |-
    We've deployed a few Jaeger services for you while setting up this exercise.
    They can be seen be running the below command.
    The Jaeger UI is also exposed in the tab, you will use it shortly to visualize some traces. <br>

    ```
    kubectl get svc --selector=app=jaeger
    ```

    Now you can deploy the tracing application and configure your intentions. <br>

    ```
    kubectl apply -f fake-service
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=fake-web -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=fake-api -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=fake-cache -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=fake-payments -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=fake-currency -o name)
    ```

    Once the pods are running, you can test the application through the `Frontend`, which is exposed over a NodePort.
    It has been configured to have a 50% error rate for one of the backend services, so run this command twice.

    ```
    curl localhost:30900 | jq
    curl localhost:30900 | jq
    ```

    Navigate to the Jaeger UI and check out the traces for the last two API requests.
    For the failed API request, it is easy to determine where it occurred. <br>

    Trace data sent by Envoy will have a `component` span tag with a value of `proxy`.
    You can see a detailed description of all the data Envoy sends [here](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/tracing#what-data-each-trace-contains). <br>

    The other spans are instrumented with the application client library for tracing.
    Compare the differences in the span tags and logs. Notice the granularity of the log events instrumented by the application's client library. <br>

    We can do a quick validation that Consul was able to apply the tracing definition to our Envoy proxy config.
    Let's check this out for our API service.

    ```
    kubectl exec $(kubectl get pod --selector=app=fake-api -o name) -c envoy-sidecar -- wget -qO- 127.0.0.1:19000/config_dump | jq '[.. |."tracing"? | select(. != null)]'
    ```

    Nice work, you just debugged a distributed app with tracing.
  notes:
  - type: text
    contents: |-
      In this exercise you will take a look at Connect's tracing capabilities. <br>

      Distributed tracing allows developers to obtain visualizations of call flows in large service oriented architectures.
      It can be invaluable in understanding serialization, parallelism, and sources of latency. <br>

      Envoy helps Connect do this in a few ways:

      * Generates request IDs and trace headers for requests as they flow through the proxies
      * Sends the generated trace spans to the tracing backends
      * Forwards the trace headers to the proxied application

      You'll explore this in this challenge using popular tracing solutions Zipkin and Jaeger. <br>
  - type: text
    contents: |-
      Connect sidecars will proxy both inbound and outbound requests, but it does not automatically know how to correlate them.
      This correlation is called `header propagation` and requires instrumentation at the application level. <br>

      Your sample app does this with `zipkin-go` libraries and the OpenTracing API.
      The application compliments the trace data sent by the proxies with useful span tags and logs for better traceability. <br>
  - type: text
    contents: |-
      You will use a purpose built app called `fake-service` to test your upstreams and trace these errors.
      The app will deploy five services in the following configuration: <br>

      * Frontend - Access to our application
      * API - gRPC API to backend services
      * Cache - Cache responses for our API
      * Payments  - Process payments
      * Currency - Do currency lookups for our payments

      The application code can be found here: https://github.com/nicholasjackson/fake-service
  tabs:
  - title: App UI
    type: service
    hostname: kubernetes
    path: /ui
    port: 30900
  - title: App - Config
    type: code
    hostname: kubernetes
    path: /root/fake-service
  - title: Jaeger UI
    type: service
    hostname: kubernetes
    path: /search?service=fake-web
    port: 31686
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s Dashboard
    type: service
    hostname: kubernetes
    path: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
    port: 8001
  - title: K8s Dashboard Token
    type: code
    hostname: kubernetes
    path: /root/dashboard-token.txt
  - title: K8s
    type: terminal
    hostname: kubernetes
  difficulty: basic
  timelimit: 900
- slug: 08-ingress
  id: ydjcyyarz7ef
  type: challenge
  title: Ingress
  teaser: Use advanced ingress capabilities.
  assignment: |-
    The ingress gateway deployed with the helm chart is available as a load balancer service. <br>

    ```
    kubectl describe svc lab-consul-ingress-gateway
    ```

    Review the configuration for ingress. <br>
    * `cat fake-web-ingress-intention.yaml`
    * `cat fake-web-ingress-gateway.hcl`
    * `cat fake-service/fake-web-ingress-router.hcl`

    Update intentions and create the virtual service. Ingress config will be supported by CRDs in the next release.

    ```
    kubectl apply -f /root/fake-web-ingress-intention.yaml
    consul config write /root/fake-web-ingress-gateway.hcl
    consul config write /root/fake-service/fake-web-ingress-router.hcl
    ```

    Access your service through the ingress gateway. <br>

    ```
    curl -s http://$(kubectl get svc lab-consul-ingress-gateway -o json | jq -r '.status.loadBalancer.ingress[0].ip'):8080/fake-service | jq
    ```

    Nice work !!
  notes:
  - type: text
    contents: Consul Ingress Gateways will allow you to route traffic into your clusters.
  tabs:
  - title: K8s Dashboard Token
    type: code
    hostname: kubernetes
    path: /root/dashboard-token.txt
  - title: K8s Dashboard
    type: service
    hostname: kubernetes
    path: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
    port: 8001
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Jaeger UI
    type: service
    hostname: kubernetes
    path: /search?service=fake-web
    port: 31686
  - title: App - Config
    type: code
    hostname: kubernetes
    path: /root/tracing
  difficulty: basic
  timelimit: 300
- slug: 09-summary
  id: neancacl0syl
  type: challenge
  title: Summary
  teaser: Sandbox mode
  assignment: |-
    Congratulations on completing this lab!!! The environment will be available for the next hour and a half.
    Click `check` when  you  are  done exploring to teardown the environment.
  notes:
  - type: text
    contents: The next assignment preserves the environment.
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s Dashboard
    type: service
    hostname: kubernetes
    path: /api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
    port: 8001
  - title: Jaeger UI
    type: service
    hostname: kubernetes
    path: /search?service=fake-web
    port: 31686
  - title: K8s Dashboard Token
    type: code
    hostname: kubernetes
    path: /root/dashboard-token.txt
  difficulty: basic
  timelimit: 5400
checksum: "14518901622021488032"
