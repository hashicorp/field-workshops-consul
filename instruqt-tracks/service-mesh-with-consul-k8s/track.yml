slug: service-mesh-with-consul-k8s-jp
id: 6cgzgzn2tteq
type: track
title: Service mesh with Consul K8s (Japanese)
teaser: 世界で最も人気のあるコンテナ・オーケストレーターであるConsulのサービスメッシュのフルパワーを発揮してください。
description: |-
  このトラックでは、K8s上でConsulのサービス メッシュを試していただけます。
  スケーリング、監視、およびトレースアプリケーションなどに触れていただけます。このトラックを完了すれば、高度なL7トラフィックパターンを習得できるでしょう。
icon: https://storage.googleapis.com/instruqt-frontend/assets/hashicorp/tracks/consul.png
tags: []
owner: hashicorp
developers:
- lance@hashicorp.com
- masa@hashicorp.com
private: false
published: true
show_timer: true
challenges:
- slug: deploy-connect
  id: i1ib8dnuybzm
  type: challenge
  title: Connectのデプロイ
  teaser: K8sにConsulクラスタを構築する
  assignment: |-
    まず、ConsulをCustom value fileを使ってK8sクラスタにHelmでデプロイし、Connectを有効にします。<br>

    Helmは GitHub からクローンされラボ環境に配置されています。
    `/root/consul-valules.yaml` をクリックして、コードエディタで Consul のデプロイメント設定を表示します。<br>

    シェル `K8s` に移動して Consul をインストールしてください。注意: Helm はこの環境では事前に初期化されています。シェルに以下のコマンドをコピーするか入力してください。

    ```
    helm install -f consul-values.yaml lab ./consul-helm --wait --debug
    ```

    ステータス出力はConsulが `DEPLOYED` であることを示しているはずです。<br>


    Consul サーバーのPodがデプロイされると、Consul UI が利用できるようになります。以下の `kubectl` コマンドで進捗状況を確認してください。もし `lab-consul-server-0` が `running` であれば、Consul UI にアクセスできるはずです。

    ```
    kubectl get pods
    ```

    <br>

    これでK8sにConsulをデプロイすることができました。続行するには、`Check` ボタンをクリックしてください。

    **オプション**

    K8s UIタブを使用してワークロードを比較したい場合は、このラボの期間中はK8sダッシュボードを使用します。
    ダッシュボードはすでに設定されているので、`skip`ボタンを使って認証することができます。<br>

    ヘルムからのConsulのデプロイの状態を見たい場合は、以下のコマンドを実行してください。<br>

    ```
    helm status lab
    ```
  notes:
  - type: text
    contents: |-
      ConnectはKubernetesと一緒に使用することで、他のポッドや外部のKubernetesサービスとのポッド通信を確保することができます。
      Envoyを起動するConnectのサイドカープロキシをクラスタ内のPodに自動的に注入（Inject)することで、Kubernetes用の設定を自動化できます。<br> <br>

      この機能はconsul-k8sプロジェクトによって提供されており、公式のConsul Helmチャートを使用して自動的にインストール、設定することができます。
  - type: text
    contents: |-
      Kubernetes上でのConsulクラスタの構築は他と同じ一般的なアーキテクチャで実行することをお勧めします。
      Kubernetes を使用することで Consul クラスタと Connect メッシュの運用を容易にするいくつかの利点があります。
      [デプロイメントガイド](https://learn.hashicorp.com/consul/datacenter-deploy/deployment-guide?utm_source=instruqt&utm_medium=k8s-track&utm_term=dg)は、Kubernetes内でConsulを実行する場合でも重要です。<br>。

      このラボでは [k3s](https://k3s.io/) という K8s の軽量版を使用しています。
      この環境は K8s のシングルノードクラスタで構成されています。
      このラボでは、[NodePorts](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport) でサービスを公開します。
      K8s ワークステーション上の Consul API へのアクセスも同様に公開されます。
  - type: image
    url: https://d33wubrfki0l68.cloudfront.net/949e085caf846f7e512f420bcbd0d1a2935e27bb/4c93c/static/img/k8s-consul-simple.png
  tabs:
  - title: Helm Config
    type: code
    hostname: kubernetes
    path: /root/consul-values.yaml
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 600
- slug: deploy-application
  id: d8at2oofwndv
  type: challenge
  title: アプリケーションのデプロイ
  teaser: アプリケーションをデプロイし公開します。
  assignment: |-
    アプリをデプロイする前に、Emojifyというアプリケーションの設定ファイル`Emojify-Config`をみてください。。
    YAMLで記述されたマニフェストみると、サービスが以下のようなアノテーションを持っています。


    ```
    consul.hashicorp.com/connect-inject
    consul.hashicorp.com/connect-service-upstreams
    ```

    他のサービスを見ても、同じアノテーションが含まれています。<br>

    これらのアノテーションは、Podがトラフィックをルーティングするために Envoy サイドカープロキシを利用することを Consul に伝えます。
    また、指定されたUpstreamのPort上に要求されたサービスのリスナーを指定します。
    Helm chartでは `connectInject` を `false` に設定しているので、アプリケーションでサイドカープロキシをデプロイするためには、上記のアノテーションを明示的に追加する必要があります。

    ```
    connectInject:
      default: false
    ```

    それではアプリケーションをデプロイしてみましょう。
    `K8s` シェルに戻って、以下のコマンドを実行してください。

    ```
    kubectl apply -f emojify/ingress.yml
    kubectl apply -f emojify/website_v1.yml
    kubectl apply -f emojify/api.yml
    kubectl apply -f emojify/cache.yml
    kubectl apply -f emojify/facebox.yml

    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-ingress -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-website -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-api -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-cache -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-facebox -o name)
    ```


    現在の通信設定（Intention）を `Consul UI`タブで確認してください。<br>
    デフォルトでは、Consul クラスタはアプリケーション間のすべてのトラフィックをDenyするように設定されています。次に、Intentionを設定してアプリケーション間のトラフィックをAllowしてみましょう。
    以下の 4 つのIntentionを作成して、マイクロサービス間の通信を有効化します。


    ```
    consul intention create -allow emojify-api emojify-facebox
    consul intention create -allow emojify-api emojify-cache
    consul intention create -allow emojify-ingress emojify-api
    consul intention create -allow emojify-ingress emojify-website
    ```

    これでIntiontionが設定されました。`Consul UI`でIntentionの状態を確認してください。
    これにより`Emojify-Web-site`タブからウェブサイトにアクセスできるはずです。
  notes:
  - type: text
    contents: |-
      このラボでは、[Emojify](https://github.com/emojify-app)という、比較的複雑なマイクロサービスアプリケーションをデプロイします。<br>

      このアプリケーションは、5つの異なるコンポーネントで構成されます。

      * Ingress - APIとWebsiteへのアクセスを提供するNginxコンテナ。
      * Website - Emojifyウェブサイトの静的コンテンツを提供します。
      * API - 機械学習のバックエンドAPIを提供します。
      * Cache - API用のキャッシュレイヤー。
      * Facebox - 写真の顔を検出して識別するための機械学習を提供します。

      このエクササイズでは、これらのサービスをConnectで簡単に接続し、監視し、拡張する方法などを行っていきます。<br>

      Consul Connectは、[Datawire's Ambassador](https://www.consul.io/docs/platform/k8s/ambassador.html)のような、高度なIngressコントローラをサポートしています。
      しかし、このラボではNginxコンテナを使って基本的な機能を紹介します。
  tabs:
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/dc1/intentions
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: Emojify - Config
    type: code
    hostname: kubernetes
    path: /root/emojify
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Emojify - Website
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  difficulty: basic
  timelimit: 600
- slug: use-the-application
  id: vqyyh0dyoy3t
  type: challenge
  title: アプリケーションにアクセスする
  teaser: デプロイされたEmojifyアプリケーションを使ってみましょう。
  assignment: |-
    Emojifyアプリを使用するには、ウェブサイトにURLを入力して画像を「絵文字化」します。使用したい画像がない場合は、以下の弊社創業者の画像をお試しください。

    ```
    https://cdn.geekwire.com/wp-content/uploads/2017/10/armon-dadgar-and-mitchell-hashimoto-630x419.jpg
    ```

    次に、`Api-Service` タブで、K8s の マニフェストに基づいて生成された Consul のサービス定義を表示します。
    また、`kubectl` を使ってサービス定義を検索することもできます。 `K8s`タブ で、以下のコマンドを実行します。<br>

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-api -o name) -c consul-connect-envoy-sidecar -- cat  /consul/connect-inject/service.hcl
    ```

    もしConsulの経験がある方は、サービス定義を手動で作成したことがあると思います。
    K8sでは、このような設定ファイルなどは以下のTemplateを指定するだけで自動生成することができます。

    ```
    template:
      metadata:
        labels:
          app: emojify-api
        annotations:
          "consul.hashicorp.com/connect-inject": "true"
          "consul.hashicorp.com/connect-service-upstreams": "emojify-facebox:8003,emojify-cache:8005"
    ```

    これでEmojifyアプリのテストが完了しました。それでは、より高度なユースケースを試してみましょう！
  notes:
  - type: text
    contents: |-
      Emojifyアプリケーションは写真の顔を絵文字化することができます。<br>
      お好きな写真を使っていただいても構いません。
  tabs:
  - title: Api -  Service
    type: code
    hostname: kubernetes
    path: /tmp/api-service.hcl
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: Emojify - Website
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  difficulty: basic
  timelimit: 600
- slug: scale-the-app
  id: 47wzmzmvhwhm
  type: challenge
  title: アプリケーションのスケーリング
  teaser: 必要に応じたスケーリングをしてみましょう。
  assignment: |-
    ML機能を提供するFaceboxサービスの数を増やすために、`replicas`を増やしてください。<br>

    先ほどのemojifyアプリの仕様で、Faceboxサービスのレプリカの数を`2`に変更します。なお、ファイル名の横にある「save」アイコンをクリックして、変更内容を保存してください。

    ```
    spec:
      replicas: 2
    ```

    次に`K8s`タブで変更を適用します。

    ```
    kubectl apply -f emojify/facebox.yml
    ```

    Consul カタログに 2 つのヘルシーな`emojify-facebox`サービスがあることを確認してみましょう。これは `Consul UI` タブで行うこともできますし、`K8s` シェルの以下の HTTP API コマンドを使用して行うこともできます。

    ```
    curl -s localhost:30085/v1/catalog/service/emojify-facebox | jq '[.. |."ServiceAddress"? | select(. != null)]'
    ```

    また、`kubectl` を使って Envoy の sidecar プロキシが動いていることを確認できます。

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-api -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/clusters | grep facebox
    ```

    これでサービスメッシュ内でダイナミックなサービスディスカバリを利用して、ワークロードをスケールアウトできました。
  notes:
  - type: text
    contents: |-
      Emojifyのアプリケーションが何をするのかは見ていただいたと思います。<br>
      次にバックエンドをスケールアウトし、サービス メッシュ上で可用性を向上してみましょう。
  tabs:
  - title: Facebox - Config
    type: code
    hostname: kubernetes
    path: /root/emojify/facebox.yml
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  difficulty: basic
  timelimit: 600
- slug: get-metrics
  id: ya40q8w8edkz
  type: challenge
  title: メトリクスの取得
  teaser: アプリケーションの状態を監視してみましょう。
  assignment: |-
    モニタリングスタックとして Grafana と Prometheus を使用します。

    ```
    kubectl get pod --selector=app=prometheus
    kubectl get pod --selector=app=grafana
    ```

    これらは以下のHelm Chartsによりインストールされます。

    * [Grafana](https://github.com/helm/charts/tree/master/stable/grafana)
    * [Prometheus](https://github.com/helm/charts/tree/master/stable/prometheus)

    GrafanaのUIには以下のクレデンシャルでログインできます。

    * username: `admin`
    * password: `Grafana - Password内のもの`

    サンプルのDashboardを利用してアプリケーションのメトリクスを見ていきましょう。<br>

    サンプルのDashboardは左側にある`+`ボタンの`Import`から指定します。
    以下のURLを別のブラウザタブで開いて、表示されたJSONをコピーしてペーストしてください。<br>

    ```
    https://raw.githubusercontent.com/hashicorp/consul-k8s-l7-obs-guide/master/overview_dashboard.json
    ```

    もし問題が起きた場合は、[Grafanaのドキュメント](https://grafana.com/docs/reference/export_import/#importing-a-dashboard) を参照してください。


    アプリケーションへのトラフィックを発生させてみます。以下のコマンドを`K8s`タブで実行してください。<br>

    ```
    kubectl apply -f emojify/traffic.yml
    ```

    しばらく待つとGrafanaのDashboardでメトリクスが表示されます。
  notes:
  - type: text
    contents: |-
      前のラボでは、アプリケーションのコンポーネントをスケールアウトしました。
       <br>

      このラボでは、マイクロサービスの詳細なTelemetryを観察してみます。
      このObservabilityはConsul Connectの強力な機能であり、アプリケーションのパフォーマンスをリアルタイムで把握することができます。<br>。

      監視系のコンポーネントを立ち上げているので１〜２分ほどお待ちください。
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Grafana - UI
    type: service
    hostname: kubernetes
    port: 30030
  - title: Grafana - Password
    type: code
    hostname: kubernetes
    path: /tmp/grafana-pass.txt
  - title: Prometheus - UI
    type: service
    hostname: kubernetes
    path: /targets#job-kubernetes-pods
    port: 30090
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 600
- slug: ship-a-new-feature
  id: 0tweijoofljz
  type: challenge
  title: 新機能の追加
  teaser: 新しいバージョンのアプリケーションのA/Bテストをしてみましょう。
  assignment: |-
    ここでは、ConnectのもつTraffic splitting機能を使ってA/Bテストを行います。<br>

    emojifyのフォルダにあるいくつかの設定ファイルをみてください。
    新しいバージョンのアプリケーションには、ユーザーが自分の写真をemojifyした後、オプションで購入できる機能を搭載します。<br>

    websiteのマニフェストのDeploymentの中に、`consul.hashicorp.com/service-tags`というConsulのアノテーションがあります。
    この`service-tags`というメタデータにより、特定の条件に基づいてウェブサイトの `v1` または `v2` のどちらかにトラフィックを送ることができるようになります。<br> <br>

    まずはConnect の L7 ルーティング設定をデプロイすることから始めます。 以下のいくつかの設定ファイルがあります。<br>

    * Resolver - Websiteのサービスのサブセットを作成します。この例では、サービスタグを使用します。

    ```
    kind           = "service-resolver"
    name           = "emojify-website"
    default_subset = "v1"
    subsets = {
      "v1" = {
        filter = "v1 in Service.Tags"
      }
      "v2" = {
        filter = "v2 in Service.Tags"
      }
    }
    ```

    * Splitter - トラフィックのSplitルールが含まれています。この例では新しいサービスにトラフィックの90％を送信します。

    ```
    kind = "service-splitter"
    name = "emojify-website"
    splits = [
      {
        weight         = 10
        service_subset = "v1"
      },
      {
        weight         = 90
        service_subset = "v2"
      },
    ]
    ```

    * Router   - 様々な条件によるトラフィックのコントロールを設定します。ここではHTTP GET APIのQuery値で制御しますが、他にもHTTPのHeaderやリクエストのPathなどでも制御できます。

    ```
    kind = "service-router"
    name = "emojify-website"
    routes = [
      {
        match {
          http {
            query_param = [
              {
                name  = "x-version"
                exact = "1"
              },
            ]
          }
        }
        destination {
          service        = "emojify-website"
          service_subset = "v1"
        }
      },
      {
        match {
          http {
            query_param = [
              {
                name  = "x-version"
                exact = "2"
              },
            ]
          }
        }
        destination {
          service        = "emojify-website"
          service_subset = "v2"
        }
      }
    ]
    ```

    それではこれらの設定を適用してみましょう。 <br>

    ```
    consul config write emojify/resolver.hcl
    consul config write emojify/splitter.hcl
    consul config write emojify/router.hcl
    ```

    設定が適用されたかは`Consul UI`の`emojify-website`サービスを開き、`Routing`というタブから確認できます。<br>

    新しいルーティングルールができたら、決済機能のついた新しいバージョンのウェブサイトをデプロイしてください。

    ```
    kubectl  apply -f emojify/website_v2.yml
    kubectl  apply -f emojify/payments.yml

    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=emojify-payments -o name)

    consul intention create -allow emojify-ingress emojify-payments
    ```

    Podが作成されReadyになったら、テストを開始できます。
    Consul UIで`emojify-website`をチェックしてみてください。Tagが追加され二つのバージョンが登録されています。<br>
    ConsulへのHTTP APIでも確認することができます。<br>

    ```
    curl -s http://127.0.0.1:8500/v1/catalog/service/emojify-website | jq
    ```

    それでゃQuery値を使ってそれぞれのバージョンへリクエストを送信してみましょう。
    レスポンスに含まれる`PAYMENT_ENABLED`の値の違いをみてください。 <br>

    ```
    curl localhost:30000/config/env.js?x-version=1
    curl localhost:30000/config/env.js?x-version=2
    ```

    それでは`Emojify A/B`タブに切り替えて何度かリフレッシュしてみてください。バナーが新しいバージョンだと「Emojify Enterprise」と表示されます。<br>

    お好きな写真を指定してEmojifyしてみましょう。以下はいくつかのサンプル写真です<br>

    ```
    https://cdn.geekwire.com/wp-content/uploads/2019/09/20190911_140446-1260x945.jpg
    ```

    ```
    https://avatars1.githubusercontent.com/u/45160975?s=460&u=6aae7ccdb56d6017bc0b119d7b723b71fd5fd2fb&v=4
    ```

    Enterpriseのバージョンだと「Buy Now!」というボタンが表示されます。適当なクレジットカード番号（本当の情報は入力しないでくださいね）を入力して購入しようとしてください。`sorry insufficient funds`ということで決済に失敗します。<br>
    Errorになるのは想定通りなので気にしないでください。<br>

    さて、Envoyの設定もみてみましょう。v1とv2の設定があります。

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-ingress -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/clusters  | grep emojify-website
    ```

    また、Envoyがルーティング定義を更新してくれていることも確認できます。<br>

    ```
    kubectl exec $(kubectl get pod --selector=app=emojify-ingress -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/config_dump | jq '[.. |."routes"? | select(. != null)][-1]'
    ```

    最後に、すべてのトラフィックを新しいバージョンへ通信するように設定します。

    ```
    cat <<-EOF > /root/emojify/splitter.hcl
    kind = "service-splitter"
    name = "emojify-website"
    splits = [
      {
        weight         = 0
        service_subset = "v1"
      },
      {
        weight         = 100
        service_subset = "v2"
      },
    ]
    EOF
    consul config write emojify/splitter.hcl
    ```

    EnvoyやConsulの状態を再度確認すると変更が適用されていることがわかります。<br>

    これで新しいバージョンのアプリケーションを安全にリリースできました。
  notes:
  - type: text
    contents: |-
      これでアプリケーションのスケールアウトが行われ、モニタリングも設定されました。次にアプリケーションに新機能を加えて、新しいバージョンをリリースしてみましょう。<br>

      このラボでは、Connectの持つL7ルーティングを使って、リリース前のA/Bテストを行ってみます。
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Emojify - Config
    type: code
    hostname: kubernetes
    path: /root/emojify
  - title: Emojify - A/B
    type: service
    hostname: kubernetes
    path: /
    port: 30000
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 900
- slug: tracing
  id: baqex0krwjmy
  type: challenge
  title: 分散エラーのトレーシング
  teaser: 分散アプリケーションのエラーフローを可視化してみよう。
  assignment: |-
    このラボのセットアップ中に、いくつかのJaegerサービスをデプロイしました。
    これらのサービスは以下のコマンドを実行することで見ることができます。
    タブにはJaeger UIも公開されていますので、それを使ってトレースを可視化してみましょう。

    ```
    kubectl get svc --selector=app=jaeger
    ```
    以下のConnect設定を使用して、トレース情報をJaegerインスタンスにルーティングするようにEnvoyを設定します。

    * [Cluster Bootstrap](https://www.consul.io/docs/connect/proxies/envoy.html#envoy_extra_static_clusters_json)
    * [Tracing Bootstrap](https://www.consul.io/docs/connect/proxies/envoy.html#envoy_tracing_json)

    ```
      {
        "connect_timeout": "3.000s",
        "dns_lookup_family": "V4_ONLY",
        "lb_policy": "ROUND_ROBIN",
        "load_assignment": {
            "cluster_name": "jaeger_9411",
            "endpoints": [
                {
                    "lb_endpoints": [
                        {
                            "endpoint": {
                                "address": {
                                    "socket_address": {
                                        "address": "jaeger-collector",
                                        "port_value": 9411,
                                        "protocol": "TCP"
                                    }
                                }
                            }
                        }
                    ]
                }
            ]
        },
        "name": "jaeger_9411",
        "type": "STRICT_DNS"
      }
    ```

    ```
      {
        "http": {
          "name": "envoy.zipkin",
          "config": {
            "collector_cluster": "jaeger_9411",
            "collector_endpoint": "/api/v1/spans",
            "shared_span_context": false
          }
        }
      }
    ```

    それでは設定を適用してみましょう。

    ```
    consul config write trace.hcl
    ```

    次にトレーシングアプリケーションをデプロイし、Intentionを設定します。


    ```
    kubectl apply -f tracing

    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=web -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=api -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=cache -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=payments -o name)
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=currency -o name)

    consul intention create -allow web api
    consul intention create -allow api cache
    consul intention create -allow api payments
    consul intention create -allow payments currency
    ```

    Podが実行されたら、NodePortを介して公開されている `Frontend` を介してアプリケーションをテストすることができます。
    バックエンドサービスの1つのエラー率が50%になるように設定されているので、このコマンドを2回実行してください。

    ```
    curl localhost:30900 | jq
    curl localhost:30900 | jq
    ```

    次に`Jaeger UI`タブに移動して、最後の2つのAPIリクエストのトレースを確認します。
    ここで失敗したAPIリクエストについて、どこで発生したかを簡単に判断することができます。<br>
    トレースが表示されない場合は、一度リフレッシュしてみてください。

    Envoy が送信するトレースデータは、`component` span タグに `proxy` という値を持つものになります。
    Envoyが送信する全てのデータの詳細な説明はこちらでご覧になれます[here](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/tracing#what-data-each-trace-contains)。<br>

    他のSpanは、トレース用のアプリケーション・クライアント・ライブラリで設定されています。
    Spanのタグとログの違いを比較してください。アプリケーションのクライアント・ライブラリで計測されたログ・イベントの粒度に注目してください。<br>

    Consul がトレース定義を Envoy プロキシ設定に適用できたか確認してみます。。
    以下のコマンドで確認してみましょう。

    ```
    kubectl exec $(kubectl get pod --selector=app=api -o name) -c consul-connect-envoy-sidecar -- wget -qO- localhost:19000/config_dump | jq '[.. |."tracing"? | select(. != null)]'
    ```

    分散アプリケーションのAPIのトレースができました。
  notes:
  - type: text
    contents: |-
      この演習では、Connectのトレース機能を見てみましょう。<br>

      分散トレースにより、開発者は大規模なサービス指向アーキテクチャにおけるコールフローを可視化できます。<br>
      Envoyは、以下の方法でConnectによる可視化を行います。

      * プロキシを通過するリクエストのリクエストIDとトレースヘッダを生成
      * 生成されたトレーススパンをトレースバックエンドに送信
      * トレースヘッダをプロキシされたアプリケーションに転送

      このラボでは、人気のあるトレースソリューションZipkinとJaegerを使っていきます。
  - type: text
    contents: |-
      コネクトサイドカーはインバウンドとアウトバウンドの両方のリクエストをプロキシしますが、それらの関連性は気にしません。<br>
      この相関関係は `header propagation` と呼ばれ、アプリケーションレベルでの設定が必要になります。<br>

      サンプルアプリは `zipkin-go` ライブラリと OpenTracing API を使ってこれを行います。
      アプリケーションは、プロキシから送られてくるトレースデータを、スパンタグとログで補完し、トレーサビリティを実現します。
  - type: text
    contents: "アップストリームをテストし、これらのエラーをトレースするために `fake-service` という名前のアプリを使います。\nこのアプリは以下の5つのサービスをデプロイします。<br>
      \n\n* Frontend - アプリケーションへのアクセス\n* API - バックエンドサービスへのgRPC API\n* Cache - APIのレスポンスのキャッシュ\n*
      Payments - 支払いの処理\n* Currency - 支払いのための通貨検索\n\nアプリケーションコードはこちらからご覧いただけます: https://github.com/nicholasjackson/fake-service"
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: App UI
    type: service
    hostname: kubernetes
    path: /ui
    port: 30900
  - title: App - Config
    type: code
    hostname: kubernetes
    path: /root/tracing
  - title: Jaeger UI
    type: service
    hostname: kubernetes
    path: /search?service=web
    port: 31686
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  difficulty: basic
  timelimit: 900
- slug: ingress
  id: xw1aadaj804x
  type: challenge
  title: Ingress
  teaser: 高度なIngress設定を試してみよう。
  assignment: |-
    ラボのセットアップ中に、以下のK8のデプロイファイルを使用してAmbassadorを設定しました。<br>

    * https://www.getambassador.io/yaml/aes-crds.yaml
    * https://www.getambassador.io/yaml/aes.yaml
    * https://www.getambassador.io/yaml/consul/ambassador-consul-connector.yaml

    [Ambassaeor tracing](https://www.getambassador.io/user-guide/tracing-tutorial-zipkin/)も有効にしたので、Jaeger UIでゲートウェイの入り口を確認することができます。<br>。

    ConsulとのIntegrationについては[こちらのガイド](https://www.getambassador.io/user-guide/consul/)を参照ください。<br>


    ConsulとのIntegrationには、Consulのカタログを照会できるAmbassadorのリゾルバーを設定する必要があります

    ```
    cat <<EOF | kubectl -n ambassador apply -f -
    ---
    apiVersion: getambassador.io/v2
    kind: ConsulResolver
    metadata:
      name: lab-consul-dc1
    spec:
      address: lab-consul-server.default.svc.cluster.local:8500
      datacenter: dc1
    EOF
    ```

    Ambassadorの準備が整いました。これでアプリケーションのマッピングを作成することができます。<br>

    ```
    cat <<EOF | kubectl apply -f -
    ---
    apiVersion: getambassador.io/v2
    kind: Mapping
    metadata:
      name: consul-web-mapping-tls
    spec:
      prefix: /web/
      service: web-sidecar-proxy
      resolver: lab-consul-dc1
      tls: ambassador-consul
      load_balancer:
        policy: round_robin
    EOF
    ```

    次に`Ambassador UI`タブでルーティング状態を確認してみてください。。<br>

    作成したマッピングを表示したり、特定のマッピングを表示したりすることもできます。
    コマンドでこれらを管理することもできます[inline](https://www.getambassador.io/reference/mappings/#mapping-resources-and-crds)。<br>

    ```
    kubectl get mappings
    kubectl describe mapping consul-web-mapping-tls
    ```

    Ambassadorはメッシュ内でプロキシに提示する独自の証明書を持っており信頼されています。
    IntegrationではそれをK8sのtls secretとして管理しています。この証明書は以下のコマンドで見ることができます。 <br>

    ```
    kubectl get secrets -n ambassador ambassador-consul-connect -o json | jq -r '.data."tls.crt"' | base64 -d | openssl x509 -text -noout
    ```

    このトラフィックを許可するように Connect に指示する必要があります。これはサービスごとに選択的に行う必要があります。

    ```
    consul intention create -allow ambassador web
    ```

    イングレスコントローラはK3のロードバランサの後ろにあることを確認してください。 <br>

    ```
    kubectl describe svc -n ambassador ambassador
    ```

    最後に、[JWT filter](https://www.getambassador.io/reference/filter-reference/#filter-type-jwt)を使ってエンドユーザー認証を設定してみましょう。
    通常、ウェブアプリケーションの場合、認証はエッジで実装され、API/エッジゲートウェイを経由するか、アプリケーションフレームワーク内のトップレベルのリクエストフィルタを経由します。
    サービスメッシュはこのような外部クライアントの制御は行わないので、別の方法で認証を行う必要があります。<br>

    JWT Filterは `web` のオーディエンスを必要とするように設定されており、JWT トークンを注入してメッシュ内のダウンストリームアプリケーションに渡します。
    デモ用に[none algorithm](https://tools.ietf.org/html/rfc7518#section-3.6)を使用しています。
    それでは適用してください。

    ```
    cat <<EOF | kubectl apply -f -
    ---
    apiVersion: getambassador.io/v2
    kind: Filter
    metadata:
      name: jwt-filter
    spec:
      JWT:
        jwksURI: "https://getambassador-demo.auth0.com/.well-known/jwks.json"
        validAlgorithms:
          - "none"
        requireAudience: yes
        audience: "web"
        injectRequestHeaders:
          - name: "X-Token-String"
            value: "{{ .token.Raw }}"
    EOF
    ```

    次に、フィルタを使用するために `/web/` ルートを設定します。

    ```
    cat <<EOF | kubectl apply -f -
    ---
    apiVersion: getambassador.io/v2
    kind: FilterPolicy
    metadata:
      name: web-policy
    spec:
      rules:
        - host: "*"
          path: /web/
          filters:
            - name: jwt-filter
    EOF
    ```

    AmbassadorのロードバランサーのIPを取得します。<br>

    ```
    ambassador=$(kubectl get svc ambassador -n ambassador -o json | jq -r '.status.loadBalancer.ingress[0].ip')
    echo $ambassador
    ```

    それでは、アプリケーションにトラフィックを送信します。
    サンプルJWTが用意されています。まずはこれをデコードしてから、APIを試してみてください。  <br>

    ```
    jwt eyJhbGciOiJub25lIiwidHlwIjoiSldUIiwiZXh0cmEiOiJzbyBtdWNoIn0.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkxhbmNlIExhcnNlbiIsImlhdCI6MTU3Nzg4MDAwMCwgImF1ZCI6ICJ3ZWIifQ.
    ```

    JWT Token認証ありとなしで試してみてください。<br>

    ```
    curl http://${ambassador}/web/
    curl -H 'Authorization: Bearer eyJhbGciOiJub25lIiwidHlwIjoiSldUIiwiZXh0cmEiOiJzbyBtdWNoIn0.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkxhbmNlIExhcnNlbiIsImlhdCI6MTU3Nzg4MDAwMCwgImF1ZCI6ICJ3ZWIifQ.' http://${ambassador}/web/
    ```

    Jaeger UIにログインして、新しいトレースを見てみましょう。
    Ambassador からの追加トラフィックが確認できます。 `ext_autz` のトレースをチェックしてください。
  notes:
  - type: text
    contents: |-
      Consulは豊富なエコシステムをサポートしており、Connectも例外ではありません。
      このラボでは、[Datawire's Ambassador](https://www.consul.io/docs/k8s/connect/ambassador) を活用します。
      前のラボのトレーシングアプリケーションへK8sのIngressを設定してみます。<br>

      Ingressは、クラスターの外部からクラスター内のサービスにHTTPとHTTPSのルートを公開します。<br>

      IngressコントローラはIngressを満たす役割を担っており、通常はロードバランサを使用します。
      ただし、アプリケーションごとに専用のロードバランサーを稼働させるには、非常にコストがかかります。
      Ingressはこの問題を効率的に管理するのに役立ちます。<br>

      アンバサダーにトラフィックを送るために[MetalLB load balancer](https://metallb.universe.tf/)を使用します。<br>
  tabs:
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Jaeger UI
    type: service
    hostname: kubernetes
    path: /search?service=ambassador-ambassador
    port: 31686
  - title: Ambassador UI
    type: service
    hostname: kubernetes
    path: /ambassador/v0/diag/
    port: 31877
  - title: Ambassador Portal
    type: service
    hostname: kubernetes
    path: /docs/
    port: 31800
  - title: App - Config
    type: code
    hostname: kubernetes
    path: /root/tracing
  difficulty: basic
  timelimit: 300
- slug: summary
  id: w9kg9wvgwxhz
  type: challenge
  title: まとめ
  teaser: Sandbox mode
  assignment: ラボ終了おめでとうございます! `check`をクリックするとこの環境は破棄されます。
  notes:
  - type: text
    contents: すべてのラボは終了です。お疲れ様でした。
  tabs:
  - title: K8s
    type: terminal
    hostname: kubernetes
  - title: Consul UI
    type: service
    hostname: kubernetes
    path: /ui/
    port: 30085
  - title: K8s UI
    type: service
    hostname: kubernetes
    port: 30443
  - title: Jaeger UI
    type: service
    hostname: kubernetes
    path: /search?service=ambassador-ambassador
    port: 31686
  - title: Ambassador UI
    type: service
    hostname: kubernetes
    path: /ambassador/v0/diag/
    port: 31877
  - title: Ambassador Portal
    type: service
    hostname: kubernetes
    path: /docs/
    port: 31800
  difficulty: basic
  timelimit: 5400
checksum: "12907457531144472509"
