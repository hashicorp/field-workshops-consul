slug: consul-enterprise-on-aws
id: stkflr7xh9ud
type: track
title: 'DEPRECATED: Consul on AWS'
teaser: 'See link for new track: https://play.instruqt.com/hashicorp/tracks/multi-cloud-service-networking-with-consul'
description: In this track you will set up a Consul shared service on AWS, and connect
  applications across multiple platforms.
icon: https://storage.googleapis.com/instruqt-frontend/assets/hashicorp/tracks/consul.png
tags: []
owner: hashicorp
developers:
- cpu@hashicorp.com
- lance@hashicorp.com
private: true
published: true
show_timer: true
skipping_enabled: true
challenges:
- slug: provision-vpcs
  id: ujbryneafql8
  type: challenge
  title: 'Infrastructure: Provision VPCs'
  teaser: Create VPCs for Consul shared service and Development teams
  assignment: |-
    You can think of the Cloud CLI terminal as your laptop machine preloaded with cloud access credentials for your company. <br>

    At any time you can use the AWS console to view your environment.
    AWS CLI commands will be provided for you to interact with AWS throughout this lab. <br>

    Your first task is setting up VPCs for each team.

    Inspect the Terraform code, and then provision the VPCs.

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    You'll notice you have three separate VPCs.

    ```
    aws ec2 describe-vpcs
    ```

    Their CIDR blocks are listed below:

    ```
    terraform-vpc-shared-svcs: 10.1.0.0/16
    terraform-vpc-frontend: 10.2.0.0/16
    terraform-vpc-backend: 10.3.0.0/16
    ```

    The shared service VPC will run the core Consul infrastructure.
    You will provision the Consul servers into this VPC in the next few assignments.
  notes:
  - type: text
    contents: |-
      In this assignment you will familiarize yourself with the AWS cloud
      environment, and provision VPCs for your services and applications.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/01-provision-vpcs.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vpc
  difficulty: basic
  timelimit: 1200
- slug: provision-core-services
  id: ixv5ankwmiwf
  type: challenge
  title: 'Infrastructure: Provision Core Services'
  teaser: A short description of the challenge.
  assignment: |-
    You will use Terraform to provision these services in the background while you set up Consul in the next few assignments. <br>

    Start with Vault. <br>

    ```
    cd /root/terraform/vault
    terraform plan
    nohup terraform apply -auto-approve > /root/terraform/vault/terraform.out &
    ```

    Next, provision EKS. <br>

    ```
    cd /root/terraform/eks
    terraform plan
    nohup terraform apply -auto-approve > /root/terraform/eks/terraform.out &
    ```

    You can continue immediately to the next assignment while these services are provisioning.
    Click the check button now to proceed.
  notes:
  - type: text
    contents: In this assignment you will provision a Vault instance and the EKS clusters
      that the development teams rely on to run their workloads.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/01-provision-vpcs.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform - EKS
    type: code
    hostname: cloud-client
    path: /root/terraform/eks
  - title: Terraform - Vault
    type: code
    hostname: cloud-client
    path: /root/terraform/vault
  difficulty: basic
  timelimit: 300
- slug: build-consul-image
  id: x3xlgqyvidby
  type: challenge
  title: 'Operations: Build Consul Image'
  teaser: Create a Consul immutable image
  assignment: |-
    The Packer build has been set up for you, which you can optionally look through in the code editor.
    This Packer build is provided by the HashiCorp implementation services team. <br>

    Inspect the variable file, then build the EC2 image.

    ```
    cat /root/packer/vars.json
    AWS_REGION=us-east-1 packer build -var-file vars.json centos.json
    ```

    Validate your AMI is available.

    ```
    aws ec2 describe-images --owners self
    ```

    Now that you have a Consul image, you're ready to provision the Consul servers in an ASG.
  notes:
  - type: text
    contents: |-
      In this assignment you will build an immutable image of Consul with [HashiCorp Packer](https://packer.io/). <br>

      Immutability has many advantages for infrastructure management.
      Consul Enterprise can take advantage of immutable patterns with [Automated Upgrades](https://www.consul.io/docs/enterprise/upgrades/index.html).
  tabs:
  - title: Packer
    type: code
    hostname: cloud-client
    path: /root/packer
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/02-build-consul-image.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: provision-consul
  id: jhvf5b2rlhmk
  type: challenge
  title: 'Operations: Provision Consul'
  teaser: Provision Consul severs in an ASG
  assignment: |-
    A terraform module has been provided for you to provision your Consul immutable image into the ASG.
    This module is provided by the HashiCorp implementation services team. <br>

    Inspect the variable file for Terraform, and deploy Consul.

    ```
    cat terraform.tfvars
    cat main.tf
    terraform plan
    terraform apply -auto-approve
    ```

    Now, validate the Consul servers deployed to your ASG. <br>

    ```
    aws autoscaling describe-auto-scaling-groups
    ```

    You should see the Consul servers with a ``*-0.0.1` postfix.
    You will increment the deployment in the next exercise to finish the immutable bootstrap.

    Let's inspect one of the newly provisioned Consul instances.
    We can access the newly provisioned instance through our bastion host helper script.

    ```
    check-consul-config /etc/consul.d/consul.hcl
    ```

    You will finish setting up the Consul servers in the next assignment.
  notes:
  - type: text
    contents: In this assignment, you'll install Consul into ASG using the immutable
      image you created in the last assignment. <br>
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/03-provision-consul.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: bootstrap-consul
  id: 7mefmboypk7h
  type: challenge
  title: 'Operations: Bootstrap Consul'
  teaser: Bootstrap Consul with an immutable migration
  assignment: |-
    In one of the terminal window, you will watch the new servers transition in real time.

    ```
    watch 'consul operator raft list-peers ; consul catalog nodes --detailed'
    ```

    You will notice two changes to the Terraform variables to complete the bootstrapping process.
      * consul_cluster_version = "0.0.2"
      * bootstrap = false

    Now run terraform, and watch the servers automatically transition with the watch command you configured earlier.

    ```
    cat terraform.tfvars
    cat main.tf
    terraform plan
    terraform apply -auto-approve
    ```

    You can also see that the `default_policy` for ACLs was updated to deny, the desired production value.

    ```
    check-consul-config /etc/consul.d/consul.hcl
    ```

    In the next assignment you will centralize secrets from the Consul bootstrap for service consumers.
  notes:
  - type: text
    contents: |-
      Consul Enterprise supports an [upgrade pattern](https://learn.hashicorp.com/consul/day-2-operations/autopilot#upgrade-migrations) that allows operators to deploy a complete cluster of new servers,
      and safely migrate the new servers until the upgrade is complete. <br>

      This feature allows you to easily apply codified changes to the Consul servers without impacting availability of the service.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/04-bootstrap-consul.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  difficulty: basic
  timelimit: 1200
- slug: validate-load-balancers
  id: yxgcduonarhs
  type: challenge
  title: 'Operations: Validate Load Balancers'
  teaser: A short description of the challenge.
  assignment: |-
    In the past few assignments you created Load Balancers for both Vault and Consul. <br>

    Your Vault LB should be ready by now. Check its health for a 200 code. <br>

    ```
    vault=$(aws elbv2 describe-load-balancers --names vault-lb --output json | jq -r .LoadBalancers[0].DNSName)
    curl -s -v http://{$vault}/v1/sys/health | jq
    ```

    Applications and services that use Consul typically do so through a Consul agent running on the same VM, K8s worker node, etc.
    In some cases, to make the UI available for example, Consul's API can also be exposed via a load balancer.
    Check the status of the cluster through the load balancer. <br>

    You can use the Consul CA file from your Terraform output in the below curl command. <br>

    ```
    consul=$(aws elbv2 describe-load-balancers --names consul-lb --output json | jq -r .LoadBalancers[0].DNSName)
    curl -s -v --cacert /root/terraform/consul/hashicorp-consul-ca-cert.pem https://{$consul}/v1/status/leader | jq
    ```

    In the next assignment you will retrieve Consul secrets from Vault.
  notes:
  - type: text
    contents: In this assignment you will validate your AWS ALBs for both Vault and
      Consul.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/04-bootstrap-consul.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 300
- slug: centralize-consul-secrets
  id: 8whoxkkzchx0
  type: challenge
  title: 'Security: Centralize Consul Secrets'
  teaser: Use Vault to store Consul secrets
  assignment: |-
    The Consul bootstrap outputs from the last few assignments have been secured in Vault through the provisioning process. <br>

    Login as an operator and inspect the credentials. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    ```

    Now inspect the credentials.

    ```
    vault kv get secret/consul
    ```

    You can use the master token to create a management token for Vault to issue [dynamic secrets](https://www.vaultproject.io/docs/secrets/consul/) for Consul.

    Get a management token for Vault to manage Consul tokens with.
    You can retrieve the privileged token for this operation from Vault.  <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault kv get -field=master_token secret/consul)
    vault_consul_mgmt_token=$(consul acl token create -policy-name=global-management -description "vault mgmt" | grep SecretID | cut -d ":" -f2 | xargs)
    ```

    Now configure the secrets engine.

    ```
    vault write consul/config/access address=${CONSUL_HTTP_ADDR} token=${vault_consul_mgmt_token}
    vault read consul/config/access
    ```

    Last, create a policy for the operations team, and link it to the Vault role.

    ```
    consul acl policy create -name "ops" -description "admin policy for ops" -rules 'acl = "write" operator = "write" namespace_prefix "" {acl = "write"}'
    vault write consul/roles/ops policies=ops ttl=1h
    ```

    Now you are ready to get a dynamic Consul token from Vault for an operator.
    Validate the token after you fetch it. <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    You can use this token to set up the anonymous policy.
    ```
    echo '
    node_prefix "" {
      policy = "read"
    }
    session_prefix "" {
      policy = "read"
    }
    agent_prefix "" {
      policy = "read"
    }
    query_prefix "" {
      policy = "read"
    }
    operator = "read"
    namespace_prefix "" {
      acl = "read"
      intention = "read"
      service_prefix "" {
        policy = "read"
      }
      node_prefix "" {
        policy = "read"
      }
    }' |  consul acl policy create -name anonymous -rules -
    consul acl token update -id anonymous -policy-name anonymous
    ```

    You will use this role in a later assignment to configure access for Consul service consumers.
  notes:
  - type: text
    contents: |-
      Terraform and Vault are commonly used together to provide secure provisioning workflows.
      Terraform Enterprise provides additional features on top of Terraform to make securing this workflow easier. <br>

      In this assignment you will use these highly privileged credentials to configure self-service,
      least privileged access to the Consul service. <br>

      A Vault instance is provisioning for this assignment. Please be patient as this can take a few minutes.
  tabs:
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vault
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/05-centralize-consul-secrets.html
  difficulty: basic
  timelimit: 1200
- slug: validate-backup-agent
  id: iq5nherggpo9
  type: challenge
  title: 'Operations: Validate Backup Agent'
  teaser: Automatically Backup Consul to S3
  assignment: |-
    Automated Backups requires a fully licensed Consul Enterprise environment to function properly.
    This environment currently has a temporary license. <br>

    If you have an enterprise license (your instructor will provide one for you) continue with the assignment.
    If you do not have an enterprise license, click `check` now and skip this section. <br>

    The Consul snapshot agent is highly available, and coordinates with other snapshot agents by establishing a [session](https://www.consul.io/docs/internals/sessions.html) in Consul.
    You will validate the session, and check S3 for the Consul snapshot backup files. <br>

    Get a privileged token to perform the license action. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    Now apply the license. <br>

    ```
    consul license put <license>
    sleep 60
    ```

    Inspect the configure file for the agent that was applied by the Terraform module. <br>

    ```
    check-consul-config /etc/consul-snapshot.d/consul-snapshot.json | jq
    ```

    The Snapshot Agent service should now be healthy.

    ```
    curl -s ${CONSUL_HTTP_ADDR}/v1/health/service/consul-snapshot?passing=true | jq
    ```

    Now you can retrieve the session that is holding the lock. <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault kv get -field snapshot_token secret/consul)
    consul acl token read -self
    consul kv get -detailed consul-snapshot/lock
    ```

    The session info holds the address of the active backup agent. <br>

    ```
    session=$(consul kv get -detailed consul-snapshot/lock | grep Session | awk -F ' ' '{print $2}')
    curl -v --header "X-Consul-Token: ${CONSUL_HTTP_TOKEN}" $CONSUL_HTTP_ADDR/v1/session/info/${session} | jq
    ```

    With a session lock present, you should have Consul backups in the S3 bucket.

    ```
    aws s3 ls
    bucket=$(aws s3api list-buckets --query "Buckets[].Name" | jq -r .[0])
    aws s3 ls s3://${bucket}/consul-snapshot/
    ```

    Consul is now automatically backing up to S3. In the next assignment you will start configuring the network.
  notes:
  - type: text
    contents: |-
      Consul Enterprise enables you to run the snapshot agent within your environment.
      Once running, the snapshot agent service operates as a highly available process that integrates with the snapshot API to automatically manage taking snapshots, backup rotation, and sending backup files offsite to Amazon S3.

      In this assignment you will validate the Snapshot Agent is working correctly.
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vault
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/05-centralize-consul-secrets.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: platform-quiz
  id: yreyomgnhyt9
  type: quiz
  title: 'Quiz: Platform'
  teaser: Platform Review
  assignment: |
    What feature(s) did you use to set up your Consul shared service in the lab?
  notes:
  - type: text
    contents: |
      Quiz time!
  answers:
  - Automated Upgrades
  - Automated Backups
  - Consul Sessions
  - None of the Above
  solution:
  - 0
  - 1
  difficulty: basic
  timelimit: 300
- slug: create-vpc-peering
  id: suzx6ufcymhe
  type: challenge
  title: 'Network: Create Transitive Peering'
  teaser: Add connectivity to the Consul shared service
  assignment: |-
    Run Terraform to setup the transit gateway routes between VPCs you provisioned earlier. <br>

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    Inspect the route table for the backend team private subnets.

    ```
    route_table=$(/usr/local/bin/terraform output -state /root/terraform/vpc/terraform.tfstate -json backend_private_route_table_ids | jq -r .[0])
    aws ec2 describe-route-tables --route-table-ids ${route_table}
    ```

    You'll notice routes to CIDR blocks for shared services route target the TGW. <br>

    In a later assignment you will use these TGW routes to connect the mesh across VPCs.
  notes:
  - type: text
    contents: |-
      [AWS Transit Gateway](https://aws.amazon.com/transit-gateway/) can help reduce complexity of VPC peering in AWS
      environments, and is commonly used to help developers connect their applications to shared services. <br>

      In this assignment you will use TGW to provide connectivity across VPCs.
  tabs:
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/08-create-vpc-peering.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/tgw
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1200
- slug: validate-eks-clusters
  id: oxwep1bahgcl
  type: challenge
  title: 'Infrastructure: Validate EKS clusters'
  teaser: Validate EKS clusters for development teams
  assignment: |-
    The EKS provisioning you kicked off earlier should now be complete. <br>

    Validate your clusters are in the correct state.
    Their status will be `ACTIVE`. <br>

    ```
    aws eks describe-cluster --name frontend
    aws eks describe-cluster --name backend
    ```

    Create a kube config file so you can access the clusters.

    ```
    KUBECONFIG=/root/terraform/eks/kubeconfig_frontend:/root/terraform/eks/kubeconfig_backend kubectl config view --merge --flatten > ~/.kube/config
    cat ~/.kube/config
    ```

    Check the worker nodes are ready on each cluster.

    ```
    kubectl config use-context eks_frontend
    kubectl get nodes -o  wide

    kubectl config use-context eks_backend
    kubectl get nodes -o wide
    ```

    In the next assignment you will configure connectivity for these clusters back to the shared Consul service.
  notes:
  - type: text
    contents: |-
      Development teams are provided with EKS clusters in their VPCs. <br>

      In this assignment you will create the kube config for the EKS clusters provisioned earlier.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/09-provision-eks-clusters.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/eks
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  difficulty: basic
  timelimit: 1200
- slug: configure-eks-cluster-segments
  id: xrckckfdqgj9
  type: challenge
  title: 'Operations: Configure EKS cluster segments'
  teaser: Add Consul network segments for hub and spoke connectivity
  assignment: |-
    The network segment config has been added to the Terraform file with a new cluster version for Consul.
    Inspect it, then apply the configuration. <br>

    ```
    cat terraform.tfvars
    cat main.tf
    terraform plan
    terraform apply -auto-approve
    ```

    Validate your network segments were applied to the Consul servers.

    ```
    check-consul-config /etc/consul.d/zz-override.json | jq
    ```

    Now that the Consul service is stood up, and connectivity is in place, Consul operators can configure the tenants for self-service.
  notes:
  - type: text
    contents: |-
      Network policies for cross-vpc traffic are restricted to proxy authenticated traffic. <br>

      Consul enterprise [network segments](https://www.consul.io/docs/enterprise/network-segments/index.html) support this hub-and-spoke topology.
  tabs:
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/10-configure-eks-cluster-segments.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1200
- slug: gvrs-quiz
  id: kjjm5qjzltq0
  type: quiz
  title: 'Quiz: GVRS'
  teaser: Visibility, Routing, and Scale Review
  assignment: |
    What feature(s) did you use to create Consul gossip segments across AWS Transit Gateway in the lab?
  notes:
  - type: text
    contents: |
      Quiz time!
  answers:
  - Advanced Federation
  - Network Segmentation
  - Redundancy Zones
  - None of the Above
  solution:
  - 1
  difficulty: basic
  timelimit: 300
- slug: create-namespaces-and-policies
  id: g7hylnrby0x9
  type: challenge
  title: 'Operations: Create Namespaces and Policies'
  teaser: Configure self-service access
  assignment: |-
    You need a privileged operator token to configure Consul for the consuming teams.
    Authenticate with Vault, and fetch a token. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    The service discovery policy will underpin all development tenants.
    This will allow the mesh to discover services across teams, however, individuals teams will control what services can connect to them from other namespaces. <br>

    ```
    consul acl policy create -name "cross-namespace-policy" -description "cross-namespace service discovery" -rules @cross-namespace.hcl
    ```

    The K8s injector policy will allow read access to the namespaces available within the Consul service.
    This is required for the Consul K8s integration to work properly. <br>

    ```
    consul acl policy create -name "k8s-injector-policy" -description "k8s injection" -rules @injector.hcl
    ```

    Now you can create the namespaces for each development team, that share the namespace discovery policy. <br>

    ```
    consul namespace write frontend-namespace.hcl
    consul namespace write backend-namespace.hcl
    ```

    You also need to create policies for low level client agent permissions for each development VPC.
    We will create a blast radius for these tokens by CIDR block. <br>

    ```
    consul acl policy create -name "frontend-agent-policy" -description "frontend agents" -rules @frontend-team-agent.hcl
    consul acl policy create -name "backend-agent-policy" -description "backend agents" -rules @backend-team-agent.hcl
    ```

    Last, create policies for the development teams to manage their own intentions.
    Intentions allow connectivity between services. <br>

    ```
    consul acl policy create -name "frontend-developer-policy" -description "frontend dev" -rules @frontend-team-developer.hcl
    consul acl policy create -name "backend-developer-policy" -description "backend dev" -rules @backend-team-developer.hcl

    ```

    In the next assignment we will link these policies to dynamic Vault roles so developers and operators can get access centrally.
  notes:
  - type: text
    contents: Consul enterprise support [namespaces](https://www.consul.io/docs/enterprise/namespaces/index.html)
      to enable network and operations teams to provide self-service for development
      teams.
  tabs:
  - title: Policies
    type: code
    hostname: cloud-client
    path: /root/policies
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/06-create-namespaces-and-policies.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: create-consul-roles
  id: r0oxqx8ait5u
  type: challenge
  title: 'Security: Create Consul Roles'
  teaser: Create Dynamic Consul roles in Vault
  assignment: |-
    Authenticate with the Consul operations role.

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    Next, link the policies. The TTLs are scoped for each task.

    ```
    vault write consul/roles/frontend-developer     policies=frontend-developer-policy    ttl=1h
    vault write consul/roles/backend-developer      policies=backend-developer-policy     ttl=1h
    ```

    Vault can just as easily store Consul tokens that are not managed dynamically.
    Create those now. <br>

    ```
    vault kv patch secret/consul k8s_injector=$(consul acl token create -description 'k8s injector' -policy-name 'k8s-injector-policy' -format=json | jq -r .SecretID)
    vault kv patch secret/consul frontend_agent_token=$(consul acl token create -description 'frontend agent' -policy-name 'frontend-agent-policy' -format=json | jq -r .SecretID)
    vault kv patch secret/consul backend_agent_token=$(consul acl token create -description 'backend agent' -policy-name 'backend-agent-policy' -format=json | jq -r .SecretID)
    ```

    You will use these roles in later assignments to seed K8s clusters with scoped access to the central service,
    as well as allow developers the ability to configure routing within their tenant.
  notes:
  - type: text
    contents: |-
      Vault is the leading solution for secrets management across multi-cloud and hybrid deployments. <br>
      In this assignment you will enable self-service access to Consul credentials for developers and operators.
  tabs:
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/07-create-consul-roles.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: configure-eks-cluster-secrets
  id: ld9gbmuj5jnr
  type: challenge
  title: 'Operations: Configure EKS cluster secrets'
  teaser: Create K8s secrets for Consul
  assignment: |-
    You need to authenticate with Vault to gain access to the Consul roles required to provide secrets for the K8s clusters.

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    A sample script has been provided to setup each K8s cluster.
    The script will perform the following tasks:

    * Create a namespace to run Consul pods (injector webhooks, daemonset, etc.)
    * Provide a Consul gossip key
    * Provide a Consul ACL token

    Inspect the setup script.

    ```
    cat /usr/local/bin/setup-k8s-consul-secrets
    ```

    Start with the frontend cluster.

    ```
    kubectl config use-context eks_frontend
    setup-k8s-consul-secrets $(vault kv get -field gossip_key secret/consul) $(vault kv get -field frontend_agent_token secret/consul) $(vault kv get -field k8s_injector secret/consul)
    kubectl  get secrets -n consul
    ```

    Repeat, for the backend cluster.

    ```
    kubectl config use-context eks_backend
    setup-k8s-consul-secrets $(vault kv get -field gossip_key secret/consul) $(vault kv get -field backend_agent_token secret/consul) $(vault kv get -field k8s_injector secret/consul)
    kubectl get secrets -n consul
    ```

    In the next assignment you will use these secrets to deploy the Consul client agents with Helm.
  notes:
  - type: text
    contents: |-
      [Consul Helm](https://www.consul.io/docs/platform/k8s/helm.html) is the HashiCorp supported method for deploying Consul into K8s environment,
      and it supports a wide array of configuration options. <br>

      In this assignment you will configure K8s secrets that will allow client agents running in K8s
      to authenticate with Consul servers in the shared service.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/11-configure-eks-cluster-secrets.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: deploy-consul-in-eks
  id: 3ewjpnxsjsmh
  type: challenge
  title: 'Operations: Deploy Consul in EKS'
  teaser: Deploy Consul in EKS with Helm
  assignment: |-
    Use the code editor to inspect the Helm chart values for each K8s cluster.
    When you have reviewed the values you can deploy each K8s cluster.

    Start with the frontend cluster.

    ```
    kubectl config use-context eks_frontend
    helm install hashicorp -f frontend-values.yaml --namespace consul ./consul-helm --debug
    kubectl get pods -n consul
    ```

    Repeat for the backend cluster.

    ```
    kubectl config use-context eks_backend
    helm install hashicorp -f backend-values.yaml --namespace consul ./consul-helm --debug
    kubectl get pods -n consul
    ```

    Check the nodes for the full working set.
    You can continue when the Frontend and Backend EKS nodes have joined the peer set. <br>

    ```
    watch consul members
    ```

    In the next assignment we will create trust between the K8s API server and the Consul shared service.
  notes:
  - type: text
    contents: |-
      The k8s clusters are now ready to connect to the Consul shared service.
      You will deploy the Consul client agents with Helm in this assignment.
  tabs:
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/12-deploy-consul-in-eks.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Helm
    type: code
    hostname: cloud-client
    path: /root/helm
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 1200
- slug: configure-k8s-consul-authentication
  id: tnkxgigifkbu
  type: challenge
  title: 'Security: Configure K8s Consul Authentication'
  teaser: Create trust between K8s clusters and Consul
  assignment: |-
    Configuring Consul auth methods is a privileged action.
    You need a Consul operator token to perform these tasks. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    The cross namespace policy you created earlier will be attached to the token.
    You need three key pieces of information to complete this task. <br>

    * K8s endpoint
    * K8s CA
    * Service Account JWT

    The helper script will assist you. Review it now. <br>

    ```
    cat /usr/local/bin/setup-k8s-consul-auth
    ```

    Run the script for each K8s cluster.

    ```
    setup-k8s-consul-auth frontend
    setup-k8s-consul-auth backend
    ```

    In the next assignment you'll use the trust relationship you just created to deploy applications into the mesh.
  notes:
  - type: text
    contents: |-
      In this assignment you will use the [K8s Consul auth method](https://www.consul.io/docs/acl/auth-methods/kubernetes.html)
      to securely introduce Consul ACL tokens to K8s pods that run applications. <br>

      [Consul auth methods](https://www.consul.io/docs/acl/acl-auth-methods.html) make it easy to securely introduce Consul ACL tokens on ephemeral platforms.
  tabs:
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/13-configure-k8s-consul-authentication.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 1200
- slug: deploy-applications
  id: dslh2b4hvc4t
  type: challenge
  title: 'Developer: Deploy Applications'
  teaser: Deploy application workloads across K8s environments
  assignment: |-
    You can view the K8s spec files for the applications in the code editor. <br>

    Start with the Frontend cluster application. <br>

    ```
    kubectl config use-context eks_frontend
    kubectl apply -f frontend
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=web -o name) --timeout=30s
    ```

    Now deploy the backend cluster applications. <br>

    ```
    kubectl config use-context eks_backend
    kubectl apply -f backend
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=api -o name) --timeout=30s
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=cache -o name) --timeout=30s
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=currency -o name) --timeout=30s
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=payments -o name) --timeout=30s
    ```

    In the next assignment we will update intentions so the new applications will receive traffic over mTLS.
  notes:
  - type: text
    contents: The K8s clusters are now ready to run application workloads.
  tabs:
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/14-deploy-applications.html
  - title: Apps
    type: code
    hostname: cloud-client
    path: /root/apps
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: update-intentions
  id: apxx5zldywqm
  type: challenge
  title: 'Developer: Update Intentions'
  teaser: Routing as a service
  assignment: |-
    The API is now available for you to test. Send some traffic to the Web frontend.

    ```
    kubectl config use-context eks_frontend
    endpoint=$(kubectl get services web -o json | jq -r .status.loadBalancer.ingress[0].hostname)
    echo $endpoint
    curl -v -s $endpoint
    ```

    You should have received a `5xx` error. mTLS is enabled by default in the cluster,
    and you have not configured any intention rules to allow routing between services. <br>


    Try to create the Consul Intentions as different development teams and compare the results.

    First, try to create the intention as a Frontend developer.

    ```
    vault login -method=userpass username=frontend password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/frontend-developer)
    consul acl token read -self
    consul intention create --allow frontend/web backend/api
    ```

    The Backend namespace is the destination service, so a developer from this team will be able to make the update. <br>

    Try this step again as Backend developer. <br>

    ```
    vault login -method=userpass username=backend password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/backend-developer)
    consul acl token read -self
    consul intention create --allow frontend/web backend/api
    consul intention create --allow backend/api backend/cache
    consul intention create --allow backend/api backend/payments
    consul intention create --allow backend/payments backend/currency
    ```

    In the next assignment you will send traffic to the Web application in the Frontend namespace.
  notes:
  - type: text
    contents: |-
      Intentions allow service mesh routing based on service identity. <br>
      The Consul shared service allows developers to control routing within their namespaces, and selectively allow traffic from other teams to their namespace. <br>

      It can take a few minutes for an ELB to resolve DNS. Please be patient while the track waits for resolution.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: App
    type: service
    hostname: cloud-client
    path: /ui
    port: 8080
  - title: Apps
    type: code
    hostname: cloud-client
    path: /root/apps
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/15-update-intentions.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: test-application
  id: jllgtf6nz3hz
  type: challenge
  title: 'Developer: Test Application'
  teaser: Put it all together
  assignment: |-
    You need to fetch the external DNS name for the AWS ELB.

    ```
    kubectl config use-context eks_frontend
    endpoint=$(kubectl get services web -o json | jq -r .status.loadBalancer.ingress[0].hostname)
    ```

    Test the application. You should receive a `200` status code.
    Take note of the IP addresses of each service.

    ```
    curl -v -s $endpoint
    ```

    Well done! You just securely connected application workloads across multiple K8s clusters!!!
  notes:
  - type: text
    contents: The Web application in the Frontend namespace is exposed via an AWS
      ELB.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: App
    type: service
    hostname: cloud-client
    path: /ui
    port: 8080
  - title: Apps
    type: code
    hostname: cloud-client
    path: /root/apps
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/15-update-intentions.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: governance-and-policy-quiz
  id: i8yzbimnbcd7
  type: quiz
  title: 'Quiz: Governance & Policy'
  teaser: Governance & Policy Review
  assignment: |
    What feature(s) did you use to enable self-service for developers?
  notes:
  - type: text
    contents: |
      Quiz time!
  answers:
  - ACLs
  - Namespaces
  - Auth Methods
  - Binding Rules
  solution:
  - 1
  difficulty: basic
  timelimit: 300
checksum: "8785023972842365072"
