slug: hcs-on-azure
id: ztymj0qpq0vy
version: 0.0.1
type: track
title: 'DEPRECATED: HCS on Azure'
teaser: 'See link for new track: https://play.instruqt.com/hashicorp/tracks/multi-cloud-service-networking-with-consul'
description: Connect containerized and non-containerized workloads with the HashiCorp
  Consul Service on Azure
icon: https://storage.googleapis.com/instruqt-frontend/assets/hashicorp/tracks/consul.png
tags:
- hcs
- consul
owner: hashicorp
developers:
- lance@hashicorp.com
- jackson.nic@gmail.com
- cpu@hashicorp.com
private: true
published: true
show_timer: true
skipping_enabled: true
challenges:
- slug: provision-azure-virtual-networks
  id: bsyydcckaasv
  type: challenge
  title: 'Infrastructure: Provision Azure Virtual Networks'
  teaser: Create basic networking in Azure
  assignment: |-
    You can think of the Cloud CLI terminal as your laptop machine preloaded with cloud access credentials for your company. <br>

    At any time you can use the Azure console to view your environment.
    Azure CLI commands will be provided for you to interact with Azure throughout this lab. <br>

    Your first task is setting up VNets for each team.

    Inspect the Terraform code, and then provision the VNets.

    ___Note:___ <br>
    Credentials injected into this environment are eventually consistent in Azure Active Directory.
    If you receive a permissions error from Azure please wait a a minute or two and try again. <br>

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    You'll notice you have three separate VNets.

    ```
    az network vnet list -g $(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name) | jq '.[] | {"cidr": .addressSpace.addressPrefixes[0], "name": .name}'
    ```

    Their CIDR blocks are listed below:

    ```
    shared-svcs-vnet: 10.1.0.0/16
    frontend-vnet: 10.2.0.0/16
    backend-vnet: 10.3.0.0/16
    ```

    The shared service VNet will run the core Consul infrastructure.
    You will provision the Consul servers into this VNet in the next few assignments.

    <br>

    ***OPTIONAL*** <br>

    If you are revisiting this content in a self-service capacity, or are a power user, you can bring your own Azure account and provision resources direclty into your subscription.
    Your credentials will be reaped from the sandbox when the track is completed or stopped. <br>

    You will be responsible for the lifecycle of these resources in your Azure account as they will be not cleaned up automatically. <br>

    ___HashiCorp instructors will not be able to debug issues in your Azure account.___ <br>

    ___If you still wish to continue with your own credentials do the following:___ <br>

    ```
    az account clear
    az login
    ```
  notes:
  - type: text
    contents: |
      Setting up your environment... Your Azure account will be ready in ~5 minutes.
      Keep an eye on the bottom right corner to know when you can get started.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vnet
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/01-provision-azure-virtual-networks.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  difficulty: basic
  timelimit: 3600
- slug: provision-core-services
  id: cmp9zoq1joar
  type: challenge
  title: 'Infrastructure: Provision Core Services'
  teaser: Provision core application shared services
  assignment: |-
    You will use Terraform to provision these services in the background while you set up Consul in the next few assignments. <br>

    ___Note:___ <br>
    Credentials injected into this environment are eventually consistent in Azure Active Directory.
    If you receive a permissions error from Azure please wait a a minute or two and try again. <br>

    Start with Vault. <br>

    ```
    cd /root/terraform/vault
    terraform plan
    nohup terraform apply -auto-approve > /root/terraform/vault/terraform.out &
    ```

    Next, provision AKS. <br>

    ```
    cd /root/terraform/aks
    terraform plan
    nohup terraform apply -auto-approve > /root/terraform/aks/terraform.out &
    ```

    Last, provision the Virtual Network Gateway. This resource can take *30 minutes to 1 hour* to provision.

    ```
    cd /root/terraform/vgw
    terraform plan
    nohup terraform apply -auto-approve > /root/terraform/vgw/terraform.out &
    ```

    You can continue immediately to the next assignment while these services are provisioning.
    Click the check button now to proceed.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/02-provision-core-services.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 3600
- slug: provision-hcs
  id: ufjqo5xsryqp
  type: challenge
  title: 'Infrastructure: Provision HCS'
  teaser: Create an HCS cluster with Terraform
  assignment: |-
    Inspect the Terraform code, and then run Terraform to provision your HCS cluster.

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    This can take 10 minutes to 15 minutes to provision.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/hcs
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/03-provision-hcs.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  difficulty: basic
  timelimit: 3600
- slug: centralize-consul-secrets
  id: 7lfblywioxsc
  type: challenge
  title: 'Security: Centralize Consul Secrets'
  teaser: Secure credentials for Consul
  assignment: |-
    The Consul bootstrap outputs from the last few assignments have been secured in Vault through the provisioning process. <br>

    The Consul & Vault endpoints are now available <br>

    ```
    echo $CONSUL_HTTP_ADDR
    echo $VAULT_ADDR
    ```

    Login as an operator and inspect the credentials. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    ```

    You need to retrieve the HCS config from HCS with the az command line extensions.

    ```
    rg=$(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name)
    az hcs get-config -g ${rg} --name hcs
    ```

    We will centralize the configuration into HashiCorp Vault so our applications and services can retrieve it.

    ```
    bootstrap_token=$(az hcs create-token --resource-group ${rg} --name hcs | jq  -r .masterToken.secretId)
    gossip_key=$(cat consul.json | jq -r '.encrypt')
    retry_join=$(cat consul.json | jq -r '.retry_join[0]')
    ca=$(cat ca.pem)
    vault kv put secret/consul/server master_token=${bootstrap_token}
    vault kv put secret/consul/shared gossip_key=${gossip_key} retry_join=$retry_join ca="${ca}"
    ```

    Now inspect the credentials.

    ```
    echo $VAULT_ADDR
    vault kv get secret/consul/server
    vault kv get secret/consul/shared
    ```

    You can use the master token to create a management token for Vault to issue [dynamic secrets](https://www.vaultproject.io/docs/secrets/consul/) for Consul.

    Get a management token for Vault to manage Consul tokens with.
    You can retrieve the privileged token for this operation from Vault.  <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault kv get -field=master_token secret/consul/server)
    vault_consul_mgmt_token=$(consul acl token create -policy-name=global-management -description "vault mgmt" | grep SecretID | cut -d ":" -f2 | xargs)
    ```

    Now configure the secrets engine.

    ```
    vault write consul/config/access address=${CONSUL_HTTP_ADDR} token=${vault_consul_mgmt_token}
    vault read consul/config/access
    ```

    Last, create a policy for the operations team, and link it to the Vault role.

    ```
    consul acl policy create -name "ops" -description "admin policy for ops" -rules 'acl = "write" operator = "write" namespace_prefix "" {acl = "write"}'
    vault write consul/roles/ops policies=ops ttl=1h
    ```

    Now you are ready to get a dynamic Consul token from Vault for an operator.
    Validate the token after you fetch it. <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    You can use this token to set up the anonymous policy.

    ```
    echo '
    node_prefix "" {
      policy = "read"
    }
    session_prefix "" {
      policy = "read"
    }
    agent_prefix "" {
      policy = "read"
    }
    query_prefix "" {
      policy = "read"
    }
    operator = "read"

    namespace_prefix "" {
      acl = "read"
      intention = "read"
      service_prefix "" {
        policy = "read"
      }
      node_prefix "" {
        policy = "read"
      }
    }' |  consul acl policy create -name anonymous -rules -
    consul acl token update -id anonymous -policy-name anonymous
    ```

    You will use this role in a later assignment to configure access for Consul service consumers.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/04-centralize-consul-secrets.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  difficulty: basic
  timelimit: 3600
- slug: validate-aks-clusters
  id: olvgj2uu5trf
  type: challenge
  title: 'Infrastructure: Validate AKS Clusters'
  teaser: Check status of AKS worker nodes
  assignment: |-
    Your AKS clusters should now be provisioned and ready.

    ```
    rg=$(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name)
    az aks list -g ${rg} | jq '.[] | {"name": .name, "status": .provisioningState}'
    ```

    First, create a merged Kube Config file from the Terraform outputs.

    ```
    terraform output frontend_kube_config > kubeconfig_frontend
    terraform output backend_kube_config > kubeconfig_backend
    KUBECONFIG=kubeconfig_frontend:kubeconfig_backend kubectl config view --merge --flatten > ~/.kube/config
    ```

    Next, check the worker nodes are ready in each cluster.
    Start with the Frontend cluster.

    ```
    kubectl config use-context frontend-aks
    kubectl get nodes -o wide
    ```

    Now check the Backend cluster.

    ```
    kubectl config use-context backend-aks
    kubectl get nodes -o wide
    ```

    You will configure these AKS clusters for Consul in the following assignments.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/07-validate-aks-clusters.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  difficulty: basic
  timelimit: 3600
- slug: create-namespaces-and-policies
  id: zklbwtiaci0u
  type: challenge
  title: 'Operations: Create Namespaces and Policies'
  teaser: Create tenancy for self-service
  assignment: |-
    You need a privileged operator token to configure Consul for the consuming teams.
    Authenticate with Vault, and fetch a token. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    The service discovery policy will underpin all development tenants.
    This will allow the mesh to discover services across teams, however, individuals teams will control what services can connect to them from other namespaces. <br>

    ```
    consul acl policy create -name "cross-namespace-policy" -description "cross-namespace service discovery" -rules @cross-namespace.hcl
    ```

    The K8s injector policy will allow read access to the namespaces available within the Consul service.
    This is required for the Consul K8s integration to work properly. <br>

    ```
    consul acl policy create -name "k8s-injector-policy" -description "k8s injection" -rules @injector.hcl
    ```

    Now you can create the namespaces for each development team, that share the namespace discovery policy. <br>

    ```
    consul namespace update -name default -default-policy-name=cross-namespace-policy
    consul namespace write frontend-namespace.hcl
    consul namespace write backend-namespace.hcl
    ```

    You also need to create policies for low level client agent permissions for each development Vnet.
    We will create a blast radius for these tokens by CIDR block. <br>

    ```
    consul acl policy create -name "frontend-agent-policy" -description "frontend agents" -rules @frontend-team-agent.hcl
    consul acl policy create -name "backend-agent-policy" -description "backend agents" -rules @backend-team-agent.hcl
    ```

    Last, create policies for the development teams to manage their own intentions.
    Intentions allow connectivity between services. <br>

    ```
    consul acl policy create -name "frontend-developer-policy" -description "frontend dev" -rules @frontend-team-developer.hcl
    consul acl policy create -name "backend-developer-policy" -description "backend dev" -rules @backend-team-developer.hcl
    ```

    In the next assignment we will link these policies to dynamic Vault roles so developers and operators can get access centrally.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/08-create-namespaces-and-policies.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Policies
    type: code
    hostname: cloud-client
    path: /root/policies
  difficulty: basic
  timelimit: 300
- slug: create-consul-roles
  id: vx2qbphc6qgi
  type: challenge
  title: 'Security: Create Consul Roles'
  teaser: Enable least privilege access for operators
  assignment: |-
    Authenticate with the Consul operations role.

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    Next, link the policies. The TTLs are scoped for each task.

    ```
    vault write consul/roles/frontend-developer     policies=frontend-developer-policy    ttl=1h
    vault write consul/roles/backend-developer      policies=backend-developer-policy     ttl=1h
    ```

    Vault can just as easily store Consul tokens that are not managed dynamically.
    Create those now. <br>

    ```
    vault kv patch secret/consul/shared k8s_injector=$(consul acl token create -description 'k8s injector' -policy-name 'k8s-injector-policy' -format=json | jq -r .SecretID)
    vault kv put secret/consul/frontend agent_token=$(consul acl token create -description 'frontend agent' -policy-name 'frontend-agent-policy' -format=json | jq -r .SecretID)
    vault kv put secret/consul/backend  agent_token=$(consul acl token create -description 'backend agent' -policy-name 'backend-agent-policy' -format=json | jq -r .SecretID)
    ```

    You will use these roles in later assignments to seed K8s clusters with scoped access to the central service,
    as well as allow developers the ability to configure routing within their tenant.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/09-create-consul-roles.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  difficulty: basic
  timelimit: 300
- slug: configure-aks-cluster-secrets
  id: gekjc0ubhf5q
  type: challenge
  title: 'Operations: Configure AKS Cluster Secrets'
  teaser: Provide bootstrap credentials
  assignment: |-
    You need to authenticate with Vault to gain access to the Consul roles required to provide secrets for the K8s clusters.

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    A sample script has been provided to setup each K8s cluster.
    The script will perform the following tasks:

    * Create a namespace to run Consul pods (injector webhooks, daemonset, etc.)
    * Provide a Consul gossip key
    * Provide a Consul ACL token

    Inspect the setup script.

    ```
    cat /usr/local/bin/setup-k8s-consul-secrets
    ```

    Start with the frontend cluster.

    ```
    rg=$(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name)
    kubectl config use-context frontend-aks
    setup-k8s-consul-secrets ${rg} hcs hashicorp-consul-cluster $(vault kv get -field gossip_key secret/consul/shared) $(vault kv get -field agent_token secret/consul/frontend) $(vault kv get -field k8s_injector secret/consul/shared) "$(vault kv get -field ca secret/consul/shared)"
    kubectl  get secrets -n consul
    ```

    Repeat, for the backend cluster.

    ```
    rg=$(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name)
    kubectl config use-context backend-aks
    setup-k8s-consul-secrets ${rg} hcs hashicorp-consul-cluster $(vault kv get -field gossip_key secret/consul/shared) $(vault kv get -field agent_token secret/consul/backend) $(vault kv get -field k8s_injector secret/consul/shared) "$(vault kv get -field ca secret/consul/shared)"
    kubectl get secrets -n consul
    ```

    In the next assignment you will use these secrets to deploy the Consul client agents with Helm.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Setup Script
    type: code
    hostname: cloud-client
    path: /usr/local/bin/setup-k8s-consul-secrets
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/10-configure-aks-cluster-secrets.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  difficulty: basic
  timelimit: 300
- slug: deploy-consul-in-aks
  id: qwpzslmpsjrf
  type: challenge
  title: 'Operations: Deploy Consul in AKS'
  teaser: Run Consul as a DaemonSet in AKS
  assignment: |-
    Use the code editor to inspect the Helm chart values for each K8s cluster.
    When you have reviewed the values you can deploy each K8s cluster.

    Start with the frontend cluster.

    ```
    kubectl config use-context frontend-aks
    helm install hashicorp -f frontend-values.yaml --namespace consul ./consul-helm --wait --debug
    kubectl get pods -n consul
    ```

    Repeat for the backend cluster.

    ```
    kubectl config use-context backend-aks
    helm install hashicorp -f backend-values.yaml --namespace consul ./consul-helm --wait --debug
    kubectl get pods -n consul
    ```

    Check the nodes for the full working set.
    You can continue when the Frontend and Backend AKS nodes have joined the peer set. <br>

    ```
    watch consul members
    ```

    In the next assignment we will create trust between the K8s API server and the Consul shared service.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Helm
    type: code
    hostname: cloud-client
    path: /root/helm
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/11-deploy-consul-in-aks.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 300
- slug: configure-k8s-consul-authentication
  id: pizuaunrknfk
  type: challenge
  title: 'Security: Configure K8s Consul Authentication'
  teaser: Create trust between AKS and Consul
  assignment: |-
    Configuring Consul auth methods is a privileged action.
    You need a Consul operator token to perform these tasks. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    The cross namespace policy you created earlier will be attached to the token.
    You need three key pieces of information to complete this task. <br>

    * K8s endpoint
    * K8s CA
    * Service Account JWT

    The helper script will assist you. Review it now. <br>

    ```
    cat /usr/local/bin/setup-k8s-consul-auth
    ```

    Run the script for each K8s cluster.

    ```
    setup-k8s-consul-auth frontend
    setup-k8s-consul-auth backend
    ```

    In the next assignment you'll use the trust relationship you just created to deploy applications into the mesh.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  - title: Setup Script
    type: code
    hostname: cloud-client
    path: /usr/local/bin/setup-k8s-consul-auth
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/12-configure-k8s-consul-authentication.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  difficulty: basic
  timelimit: 300
- slug: deploy-applications-in-aks
  id: aayejvgkybat
  type: challenge
  title: 'Developer: Deploy Applications in AKS'
  teaser: Run containerized workloads
  assignment: |-
    You can view the K8s spec files for the applications in the code editor. <br>

    Start with the Frontend cluster application. <br>

    ```
    kubectl config use-context frontend-aks
    kubectl apply -f frontend
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=web -o name) --timeout=30s
    ```

    Now deploy the backend cluster applications. <br>

    ```
    kubectl config use-context backend-aks
    kubectl apply -f backend
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=api -o name) --timeout=30s
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=cache -o name) --timeout=30s
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=currency -o name) --timeout=30s
    ```

    In the next assignment we will update intentions so the new applications will receive traffic over mTLS.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Apps
    type: code
    hostname: cloud-client
    path: /root/apps
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/13-deploy-applications.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  difficulty: basic
  timelimit: 500
- slug: configure-vm-consul-authentication
  id: nuxxip25vfap
  type: challenge
  title: 'Security: Configure VM Authentication with Consul'
  teaser: Create trust between Azure MSI & Consul
  assignment: |-
    In this assignment you will set up authentication for the VM that runs the non-container service for Consul & Vault. <br>

    First let's create an identity for the Payments VM in Azure.

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    Login to Consul & Vault.

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    Create the policy in Consul for the VM and associate it to the role.

    ```
    consul acl policy create \
      -name "payments-policy" \
      -description "Policy for API service to grant agent permissions and consul connect integration" \
      -rules @/root/policies/payments.hcl
    consul acl role create \
      -name "payments-role" \
      -description "Role for the API service" \
      -policy-name "payments-policy"
    ```

    Next, configure Consul to accept JWTs from the Azure MSI service.

    ```
    cat <<EOF > ./jwt_auth_config.json
    {
      "BoundAudiences": [
        "https://management.azure.com/"
      ],
      "BoundIssuer": "https://sts.windows.net/${ARM_TENANT_ID}/",
      "JWKSURL":"https://login.microsoftonline.com/${ARM_TENANT_ID}/discovery/v2.0/keys",
      "ClaimMappings": {
          "id": "xms_mirid"
      }
    }
    EOF
    consul acl auth-method create -name azure -type jwt -config @jwt_auth_config.json
    consul acl binding-rule create -method=azure -bind-type=role -bind-name=payments-role -selector='value.xms_mirid matches `.*/payments`'
    ```

    Now that we have a user identity we configure Vault to trust this VM so it can retrieve the gossip key required to bootstrap Consul.
    This process will be fully automated with native Consul auth in the next release.

    ```
    vault write auth/azure/role/consul \
        policies="frontend" \
        bound_service_principal_ids=$(terraform output payments_identity_principal_id) \
        ttl=8h
    vault read auth/azure/role/consul
    ```
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/iam
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  difficulty: basic
  timelimit: 500
- slug: deploy-applications-in-vms
  id: wkplyhw7h0oo
  type: challenge
  title: 'Developer: Deploy Applications on VMs'
  teaser: Deploy VM workloads
  assignment: |-
    Now that you have set up secure introduction you can deploy the VMs. <br>

    You can review the cloud init script in the tab. <br>

    After you have reviewed the init script, deploy the VM with Terraform.

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    Wait a minute or two and you will see the payments VM added as a Consul node.

    ```
    consul members
    ```

    Now that you have the services running on AKS and on VMs you need to configure the transitive routing across VNets.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vm
  - title: Cloud Init
    type: code
    hostname: cloud-client
    path: /root/terraform/vm/scripts/vm.sh
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  difficulty: basic
  timelimit: 500
- slug: validate-virtual-gateway-and-peering
  id: hufkoyrn9f12
  type: challenge
  title: 'Network: Validate Virtual Gateway & Peering'
  teaser: Enable virtual gateway for hub and spoke routing
  assignment: |-
    The VGW and VNet peerings provisioned earlier should be close to provisioned. <br>

    Inspect the peerings, and then we will check the status of the gateway.

    ```
    rg=$(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name)
    az network vnet peering list --resource-group ${rg} --vnet-name shared-svcs-vnet | jq '.[] | {"name": .name, "state": .peeringState}'
    az network vnet peering list --resource-group ${rg} --vnet-name frontend-vnet | jq '.[] | {"name": .name, "state": .peeringState}'
    az network vnet peering list --resource-group ${rg} --vnet-name backend-vnet | jq '.[] | {"name": .name, "state": .peeringState}'
    ```

    Before you can continue you need the VGW to finish provisioning.
    Monitor the terraform code for an updated status.

    ```
    tail -f /root/terraform/vgw/terraform.out
    ```

    You can check the state of the gateway with the below command.

    ```
    rg=$(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name)
    az network vnet-gateway list --resource-group ${rg} | jq '.[0].provisioningState'
    ```

    In a later assignment you will use the VNet peers to enable remote gateways for the spoke VNets.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/05-validate-virtual-gateway-and-peering.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  difficulty: basic
  timelimit: 3600
- slug: create-remote-gateways
  id: ug6c6cq6fyxd
  type: challenge
  title: 'Network: Create Remote Gateways'
  teaser: Connect spoke VNets to the Hub
  assignment: |-
    You can now enable spoke VNets to use the shared service gateway.

    Inspect the variable configuration and apply this now.

    ```
    cat terraform.tfvars
    terraform plan
    terraform apply -auto-approve
    ```

    Gateway transit is enabled in the shared services VNet.

    ```
    rg=$(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name)
    az network vnet peering list --resource-group ${rg} --vnet-name shared-svcs-vnet | jq '.[0].allowGatewayTransit'
    ```

    Spoke VNets are enabled to use the central transit hub.

    ```
    rg=$(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name)
    az network vnet peering list --resource-group ${rg} --vnet-name frontend-vnet | jq '.[0].useRemoteGateways'
    az network vnet peering list --resource-group ${rg} --vnet-name backend-vnet | jq '.[0].useRemoteGateways'
    ```

    You will also notice that each AKS VNet has a user defined route to use the remote gateway for cross VNet traffic.
    You can see this in the `nextHopType` value.

    ```
    rg=$(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name)
    az network route-table show -g ${rg} -n frontend-shared-aks | jq '.routes'
    az network route-table show -g ${rg} -n backend-shared-aks  | jq '.routes'
    ```

    Now that you have transitive routes in place you can configure the AKS clusters for the Consul deployment.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/06-create-remote-gateways.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  difficulty: basic
  timelimit: 3600
- slug: update-intentions
  id: t11vsafayxq1
  type: challenge
  title: 'Developer: Update Intentions'
  teaser: Apply mTLS rules
  assignment: |-
    The API is now available for you to test. Send some traffic to the Web frontend.

    ```
    kubectl config use-context frontend-aks
    endpoint=$(kubectl get services web -o json | jq -r .status.loadBalancer.ingress[0].ip)
    echo $endpoint
    curl -v -s $endpoint
    ```

    You should have received a `5xx` error. mTLS is enabled by default in the cluster,
    and you have not configured any intention rules to allow routing between services. <br>

    As a backend developer retrieve a Consul token and allow traffic into your namespace. <br>

    ```
    vault login -method=userpass username=backend password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/backend-developer)
    consul acl token read -self
    consul intention create --allow frontend/web backend/api
    consul intention create --allow backend/api backend/cache
    consul intention create --allow frontend/payments backend/currency
    ```

    As a frontend developer retrieve a Consul token and allow traffic into your namespace. <br>

    ```
    vault login -method=userpass username=frontend password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/frontend-developer)
    consul acl token read -self
    consul intention create --allow backend/api frontend/payments
    ```

    In the next assignment you will test the application with the mTLS rules you created.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/14-update-intentions.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: App
    type: service
    hostname: cloud-client
    path: /ui
    port: 9090
  difficulty: basic
  timelimit: 300
- slug: test-application
  id: kpys2gyuzhal
  type: challenge
  title: 'Developer: Test Application'
  teaser: Hello World from HCS & Azure
  assignment: |-
    You need to fetch the external DNS name for the Azure LB.

    ```
    kubectl config use-context frontend-aks
    endpoint=$(kubectl get services web -o json | jq -r .status.loadBalancer.ingress[0].ip)
    echo $endpoint
    ```

    Test the application. You should receive a `200` status code.
    Take note of the IP addresses of each service.

    ```
    curl -v -s $endpoint
    ```

    Well done! You just securely connected application workloads across multiple K8s clusters!!!
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/15-test-application.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/hcs-on-azure/assets/diagrams/16-final-architecture-hcs.html
  - title: Cloud  CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: App
    type: service
    hostname: cloud-client
    path: /ui
    port: 9090
  difficulty: basic
  timelimit: 300
checksum: "5459383659083362718"
